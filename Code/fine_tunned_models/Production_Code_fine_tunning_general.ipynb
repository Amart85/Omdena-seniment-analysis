{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Production_Code_fine_tunning_general.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdD8MmEtMYUv"
      },
      "source": [
        "!pip install optuna==2.3.0\n",
        "!pip install transformers==4.2.1\n",
        "!pip install farasapy\n",
        "!pip install pyarabic\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFtDemzeXK_8"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n",
        "\n",
        "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer\n",
        "from transformers.data.processors import SingleSentenceClassificationProcessor\n",
        "from transformers import Trainer , TrainingArguments\n",
        "from transformers.trainer_utils import EvaluationStrategy\n",
        "from transformers.data.processors.utils import InputFeatures\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.utils import resample\n",
        "import logging\n",
        "import torch\n",
        "import optuna "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IYJ42tQ318w"
      },
      "source": [
        "# (1)load libraries \n",
        "import json, sys, regex\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix\n",
        "##----------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "##------------------------------------------------------\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM5aKhf4Mlfv"
      },
      "source": [
        "class Dataset:\n",
        "    def __init__(\n",
        "        self,\n",
        "        name,\n",
        "        train,\n",
        "        test,\n",
        "        label_list,\n",
        "    ):\n",
        "        self.name = name\n",
        "        self.train = train\n",
        "        self.test = test\n",
        "        self.label_list = label_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzA1zxZ7NE2l"
      },
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, text, target, model_name, max_len, label_map):\n",
        "      super(BERTDataset).__init__()\n",
        "      self.text = text\n",
        "      self.target = target\n",
        "      self.tokenizer_name = model_name\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "      self.max_len = max_len\n",
        "      self.label_map = label_map\n",
        "      \n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.text)\n",
        "\n",
        "    def __getitem__(self,item):\n",
        "      text = str(self.text[item])\n",
        "      text = \" \".join(text.split())\n",
        "\n",
        "\n",
        "        \n",
        "      input_ids = self.tokenizer.encode(\n",
        "          text,\n",
        "          add_special_tokens=True,\n",
        "          max_length=self.max_len,\n",
        "          truncation='longest_first'\n",
        "      )     \n",
        "    \n",
        "      attention_mask = [1] * len(input_ids)\n",
        "\n",
        "      # Zero-pad up to the sequence length.\n",
        "      padding_length = self.max_len - len(input_ids)\n",
        "      input_ids = input_ids + ([self.tokenizer.pad_token_id] * padding_length)\n",
        "      attention_mask = attention_mask + ([0] * padding_length)    \n",
        "      \n",
        "      return InputFeatures(input_ids=input_ids, attention_mask=attention_mask, label=self.label_map[self.target[item]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-gw9eXUPAav"
      },
      "source": [
        "class BERTDatasetTest(Dataset):\n",
        "    def __init__(self, text, model_name, max_len, label_map):\n",
        "      super(BERTDataset).__init__()\n",
        "      self.text = text\n",
        "      self.tokenizer_name = model_name\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "      self.max_len = max_len\n",
        "      self.label_map = label_map\n",
        "      \n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.text)\n",
        "\n",
        "    def __getitem__(self,item):\n",
        "      text = str(self.text[item])\n",
        "      text = \" \".join(text.split())\n",
        "\n",
        "\n",
        "        \n",
        "      input_ids = self.tokenizer.encode(\n",
        "          text,\n",
        "          add_special_tokens=True,\n",
        "          max_length=self.max_len,\n",
        "          truncation='longest_first'\n",
        "      )     \n",
        "    \n",
        "      attention_mask = [1] * len(input_ids)\n",
        "\n",
        "      # Zero-pad up to the sequence length.\n",
        "      padding_length = self.max_len - len(input_ids)\n",
        "      input_ids = input_ids + ([self.tokenizer.pad_token_id] * padding_length)\n",
        "      attention_mask = attention_mask + ([0] * padding_length)    \n",
        "      \n",
        "      return InputFeatures(input_ids=input_ids, attention_mask=attention_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsddGpRf61WO"
      },
      "source": [
        "# MODEL_PATH_BEGIN_FINETUNE='/content/MARBERT_pytorch_verison'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqtS9ym3c_RL"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print (\"your device \", device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ewzCNgb3-dS"
      },
      "source": [
        "class SentimentIdentification(object):\n",
        "  \"\"\"A class for finetunning, evaluating and running the sentiment classification on hugging face model\n",
        "     After initializing an instance, you must\n",
        "    run the train method once before using it.\n",
        "    Args:\n",
        "        labels (:obj:`set` of :obj:`str`, optional): The set of sentiment labels\n",
        "            used in the training data in the main model.\n",
        "            If None, the default labels are used.\n",
        "            Defaults to None.\n",
        "\n",
        "        max_seq_length (:obj:`int`, optional): maximum sequence length for the model\n",
        "            \n",
        "            If None, the default max_seq_length are used.\n",
        "            Defaults to 256 .\n",
        "\n",
        "        num_epoch (:obj:`int`, optional): number of epoch used for training the model\n",
        "            \n",
        "            If None, the default num_epoch are used.\n",
        "            Defaults to 3 .\n",
        "\n",
        "        batch_size (:obj:`int`, optional):batch size used for training the model            \n",
        "            If None, the default batch_size are used.\n",
        "            Defaults to 16 .\n",
        "\n",
        "        lr (:obj:`int`, optional):initial learning rate used for training the model            \n",
        "            If None, the default lr are used.\n",
        "            Defaults to 5e-5 .\n",
        "\n",
        "        model_path (:obj:`str`, optional): path of model you want to re-fine tune on new dataset after saving\n",
        "        Defaults to None.\n",
        "\n",
        "\n",
        "        model_name (:obj:`str`, optional): name og hugging face model you want to fine tune\n",
        "        Default: Qarib\n",
        "        \n",
        "       \n",
        "    \"\"\"\n",
        "  def __init__(self, labels=None,max_seq_length=256,num_epoch=3,batch_size=16,lr=5e-5, model_path=None,model_name='qarib/bert-base-qarib'\n",
        "                 ):\n",
        "        if labels is None:\n",
        "            self.labels = _DEFAULT_LABELS\n",
        "        self._labels_sorted = sorted(labels)\n",
        "        self._is_trained = False\n",
        "        self.model_name=model_name\n",
        "        self.task='classification'\n",
        "        self.num_epoch=num_epoch\n",
        "        self.batch_size=batch_size\n",
        "        self.max_seq_length=max_seq_length\n",
        "        self.lr=lr\n",
        "        self.model_path=model_path\n",
        "  def create_label2ind_file(self):\n",
        "\n",
        "    self.label_map = { v:index for index, v in enumerate(self._labels_sorted) }\n",
        "\n",
        "  def save_label2ind_file(self,path):\n",
        "    \"\"\"Save  the label 2 indexr on a given data set.\n",
        "      Args:\n",
        "          Path (:obj:`str`): Path where you want to save the feature vector .\n",
        "              \n",
        "          \n",
        "      \"\"\"\n",
        "    with open(path, 'w') as json_file:\n",
        "        json.dump(self.label_map, json_file)\n",
        "\n",
        "  def data_prepare_BERT(self,X_train,y_train):\n",
        "\n",
        "    train_dataset = BERTDataset(X_train.to_list(),y_train.to_list(),self.model_name,self.max_seq_length,self.label_map)\n",
        "    \n",
        "      \n",
        "    return train_dataset\n",
        "  def data_prepare_BERT_test(self,X_test):\n",
        "\n",
        "    test_dataset = BERTDatasetTest(X_test.to_list(),self.model_name,self.max_seq_length,self.label_map)\n",
        "    \n",
        "    return test_dataset\n",
        "\n",
        "  def model_init(self):\n",
        "    if self.model_path not None:\n",
        "      return AutoModelForSequenceClassification.from_pretrained(self.model_path, return_dict=True, num_labels=len(self.label_map))\n",
        "    else:\n",
        "\n",
        "      return AutoModelForSequenceClassification.from_pretrained(self.model_name, return_dict=True, num_labels=len(self.label_map))\n",
        "  \n",
        "  def compute_metrics(self,p):\n",
        "\n",
        "    #p should be of type EvalPrediction\n",
        "\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    assert len(preds) == len(p.label_ids)\n",
        "    #print(classification_report(p.label_ids,preds))\n",
        "    #print(confusion_matrix(p.label_ids,preds))\n",
        "\n",
        "    macro_f1_pos_neg = f1_score(p.label_ids,preds,average='macro',labels=[0,1])\n",
        "    macro_f1 = f1_score(p.label_ids,preds,average='macro')\n",
        "    macro_precision = precision_score(p.label_ids,preds,average='macro')\n",
        "    macro_recall = recall_score(p.label_ids,preds,average='macro')\n",
        "    acc = accuracy_score(p.label_ids,preds)\n",
        "    return {\n",
        "        'macro_f1' : macro_f1,\n",
        "        'macro_f1_pos_neg' : macro_f1_pos_neg,  \n",
        "        'macro_precision': macro_precision,\n",
        "        'macro_recall': macro_recall,\n",
        "        'accuracy': acc\n",
        "    }\n",
        "\n",
        "  \n",
        "  def save_model(self,path):   \n",
        "    \"\"\"Save  the model on a given data set.\n",
        "        Args:\n",
        "            Path (:obj:`str`): Path where you want to save the model.\n",
        "               \n",
        "           \n",
        "        \"\"\"\n",
        "    self.trainer.save_model(path + '/')\n",
        "\n",
        "  def eval(self,X_eval,y_eval, data_set='DEV'):\n",
        "\n",
        "    \"\"\"Evaluate the trained model on a given data set.\n",
        "        Args:\n",
        "            X_eval (:obj:`np array or pandas series`, optional): loaded data for evaluation.\n",
        "\n",
        "            y_eval (:obj:`np array or pandas series`, optional): loaded labels for evaluation.\n",
        "\n",
        "            data_set (:obj:`str`, optional): Name of the provided data set to\n",
        "                use. This is ignored if data_path is not None. Can be either\n",
        "                'VALIDATION' or 'TEST'. Defaults to 'VALIDATION'.\n",
        "\n",
        "            batch_size (:obj:`int`, optional):batch size used for training the model            \n",
        "            If None, the default batch_size are used.\n",
        "            Defaults to 16 .\n",
        "        Returns:\n",
        "            :obj:`dict`: A dictionary mapping an evaluation metric to its\n",
        "            computed value. The metrics used are accuracy, f1_micro, f1_macro,\n",
        "            recall_micro, recall_macro, precision_micro and precision_macro.\n",
        "        \"\"\"\n",
        "    validation_inputs = self.data_prepare_BERT(X_eval,y_eval)\n",
        "    predictions=self.trainer.predict(validation_inputs)\n",
        "    all_pred=np.argmax(predictions[0],axis=1)\n",
        "    all_label= [self.label_map[i] for i in y_eval]    \n",
        "    accuracy = accuracy_score(all_label, all_pred)\n",
        "    macro_f1_pos_neg = f1_score(all_label, all_pred,average='macro',labels=[0,1])\n",
        "    f1score = f1_score(all_label, all_pred, average='macro') \n",
        "    recall = recall_score(all_label, all_pred, average='macro')\n",
        "    precision = precision_score(all_label, all_pred, average='macro')\n",
        "    # Get scores\n",
        "    scores = {\n",
        "        'Sentiment': {\n",
        "            'accuracy': accuracy,\n",
        "            'f1_macro': f1score,\n",
        "            'recall_macro': recall,\n",
        "            'precision_macro':precision\n",
        "        }\n",
        "    }\n",
        "    return scores\n",
        "\n",
        "  def predict(self,sentences):\n",
        "    \"\"\"Predict the sentiment  probability scores for a given list of\n",
        "        sentences.\n",
        "        Args:\n",
        "            sentences (:obj:`list` of :obj:`str`): The list of sentences.\n",
        "            output (:obj:`str`): The output label type. Possible values are\n",
        "                'postive', 'neagtive', 'neutral'.\n",
        "        Returns:\n",
        "            :obj:`list` of :obj:`sentIDPred`: A list of prediction results,\n",
        "            each corresponding to its respective sentence.\n",
        "        \"\"\"\n",
        "    if isinstance(sentences, str):\n",
        "      sentences=pd.Series(sentences)\n",
        "    validation_inputs = self.data_prepare_BERT_test(sentences)\n",
        "    predictions=self.trainer.predict(validation_inputs)\n",
        "    probabilities=predictions[0]\n",
        "    predicted = np.argmax(predictions[0],axis=1)   \n",
        "    result = collections.deque()\n",
        "    convert = lambda x: x     \n",
        "    for i in range(0,len(predicted)):\n",
        "      for j, val in enumerate(self.label_map):\n",
        "      \n",
        "        if j==predicted[i]:\n",
        "          result.append(convert(SentIDPred(val, probabilities[i])))\n",
        "          break\n",
        "\n",
        "        \n",
        "      \n",
        "    return list(result)    \n",
        "       \n",
        "\n",
        "         \n",
        "\n",
        "  def fine_tune(self,X_train,y_train,X_valid=None,y_valid=None):\n",
        "\n",
        "    \"\"\" fine tune MARBERT model.\n",
        "        Args:\n",
        "            X_train (:obj:`np array or pandas series`, optional): loaded training data.\n",
        "               \n",
        "            y_train (:obj:`np array or pandas series`, optional): loaded labels for training.\n",
        "\n",
        "            X_valid (:obj:`np array or pandas series`, optional): loaded validation data.\n",
        "               \n",
        "            y_valid (:obj:`np array or pandas series`, optional): loaded labels for validation.\n",
        "       \n",
        "        \"\"\"\n",
        "\n",
        "    self.create_label2ind_file()\n",
        "    if X_valid is None and y_valid is None:\n",
        "      msk = np.random.rand(len(X_train)) < 0.8\n",
        "      X_valid = X_train[~msk]\n",
        "      X_train = X_train[msk]\n",
        "      y_valid = y_train[~msk]\n",
        "      y_train = y_train[msk]\n",
        "    #-------------------------------------------------------\n",
        "    train_inputs = self.data_prepare_BERT(X_train,y_train)\n",
        "    validation_inputs = self.data_prepare_BERT(X_valid,y_valid)\n",
        "      \n",
        "    #-------------------------------------------------------\n",
        "    training_args = TrainingArguments(\"./train\")\n",
        "    training_args.evaluate_during_training = True\n",
        "    training_args.adam_epsilon = 1e-8\n",
        "    training_args.learning_rate = self.lr\n",
        "    training_args.fp16 = True\n",
        "    training_args.per_device_train_batch_size = self.batch_size\n",
        "    training_args.per_device_eval_batch_size = self.batch_size\n",
        "    training_args.gradient_accumulation_steps = 2\n",
        "    training_args.num_train_epochs= self.num_epoch\n",
        "    steps_per_epoch = (len(X_train)// (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps))\n",
        "    total_steps = steps_per_epoch * training_args.num_train_epochs\n",
        "    \n",
        "    #Warmup_ratio\n",
        "    warmup_ratio = 0.1\n",
        "    training_args.warmup_steps = total_steps*warmup_ratio # or you can set the warmup steps directly \n",
        "\n",
        "    training_args.evaluation_strategy = EvaluationStrategy.EPOCH\n",
        "    # training_args.logging_steps = 200\n",
        "    training_args.save_steps = 100000 #don't want to save any model, there is probably a better way to do this :)\n",
        "    training_args.seed = 42\n",
        "    training_args.disable_tqdm = False\n",
        "    training_args.lr_scheduler_type = 'cosine'\n",
        "\n",
        "    #-------------------------------------------------------\n",
        "    self.trainer = Trainer(\n",
        "    model = self.model_init(),\n",
        "    args = training_args,\n",
        "    train_dataset = train_inputs,\n",
        "    eval_dataset=validation_inputs,\n",
        "    compute_metrics=self.compute_metrics,\n",
        ")\n",
        "\n",
        "    #------------------------------------------\n",
        "    self.trainer.train()\n",
        "    \n",
        "    #---------------------------------------------------\n",
        "    \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbSE86REiRYt"
      },
      "source": [
        "import collections\n",
        "class SentIDPred(collections.namedtuple('SentimentPred', ['top', 'scores'])):\n",
        "    \"\"\"A named tuple containing sentiment ID prediction results.\n",
        "    Attributes:\n",
        "        top (:obj:`str`): The sentiment label with the highest score. See\n",
        "            :ref:`sentimentid_labels` for a list of output labels.\n",
        "        scores (:obj:`dict`): A dictionary mapping each sentiment label to it's\n",
        "            computed score.\n",
        "    \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIqttStecsh7"
      },
      "source": [
        "class SentimentIdentificationPrediction(object):\n",
        "  \"\"\"A class for running a fine-tuned sentiment analysis model to predict\n",
        "    the sentiment of given sentences.\n",
        "\n",
        "\n",
        "    Args:\n",
        "        labels (:obj:`set` of :obj:`str`, optional): The set of dialect labels\n",
        "            used in the training data in the main model.\n",
        "            If None, the default labels are used.\n",
        "            Defaults to None.\n",
        "\n",
        "        training_model_path (:obj:`str`, optional): Path of training model to be used for inference,\n",
        "        If none, use defult model for this libaray\n",
        "\n",
        "        label2index (:obj:`str`, optional): Path of label 2 indexx file to be used for scoring,\n",
        "        If none, use defult model for this libaray\n",
        "\n",
        "        max_seq_length (:obj:`int`, optional): maximum sequence length for the model\n",
        "            \n",
        "            If None, the default max_seq_length are used.\n",
        "            Defaults to 256 .\n",
        "\n",
        "      model_name (:obj:`str`, optional): name og hugging face model you want to fine tune\n",
        "        Default: Qarib\n",
        "\n",
        "    \"\"\"\n",
        "  def __init__(self, labels=None,training_model_path=None,label2index=None,max_seq_length=256,\n",
        "                batch_size=16,model_name='qarib/bert-base-qarib' ):\n",
        "        if labels is None:\n",
        "            self.labels = _DEFAULT_LABELS\n",
        "        self._labels_sorted = sorted(labels)\n",
        "        self.model_name='qarib/bert-base-qarib'\n",
        "        self.task='classification'\n",
        "        self.batch_size=batch_size\n",
        "        self.max_seq_length=max_seq_length\n",
        "        if label2index is None:\n",
        "          self.label_map = json.load(open(LABEL_2_INDEX_PATH))\n",
        "        else:\n",
        "          self.label_map =  json.load(open(label2index))\n",
        "\n",
        "        if training_model_path is None:\n",
        "\n",
        "          self.trainer = Trainer(\n",
        "            model = self.model_init(MODEL_PATH_),\n",
        "            # args = training_args,\n",
        "            # train_dataset = train_dataset,\n",
        "            # eval_dataset=test_dataset,\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "          \n",
        "        else:\n",
        "           self.trainer = Trainer(\n",
        "            model = self.model_init(training_model_path),\n",
        "            # args = training_args,\n",
        "            # train_dataset = train_dataset,\n",
        "            # eval_dataset=test_dataset,\n",
        "            compute_metrics=self.compute_metrics,\n",
        "        )\n",
        "  def model_init(self,Path=None):\n",
        "    if Path is None:\n",
        "      return AutoModelForSequenceClassification.from_pretrained(self.model_name, return_dict=True, num_labels=len(self.label_map)) \n",
        "    else:\n",
        "      return AutoModelForSequenceClassification.from_pretrained(Path, return_dict=True, num_labels=len(self.label_map)) \n",
        "  def compute_metrics(self,p):\n",
        "\n",
        "    #p should be of type EvalPrediction\n",
        "\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    assert len(preds) == len(p.label_ids)\n",
        "    #print(classification_report(p.label_ids,preds))\n",
        "    #print(confusion_matrix(p.label_ids,preds))\n",
        "\n",
        "    macro_f1_pos_neg = f1_score(p.label_ids,preds,average='macro',labels=[0,1])\n",
        "    macro_f1 = f1_score(p.label_ids,preds,average='macro')\n",
        "    macro_precision = precision_score(p.label_ids,preds,average='macro')\n",
        "    macro_recall = recall_score(p.label_ids,preds,average='macro')\n",
        "    acc = accuracy_score(p.label_ids,preds)\n",
        "    return {\n",
        "        'macro_f1' : macro_f1,\n",
        "        'macro_f1_pos_neg' : macro_f1_pos_neg,  \n",
        "        'macro_precision': macro_precision,\n",
        "        'macro_recall': macro_recall,\n",
        "        'accuracy': acc\n",
        "    }\n",
        "\n",
        "        \n",
        "  def data_prepare_BERT(self,X_train,y_train):\n",
        "\n",
        "    train_dataset = BERTDataset(X_train.to_list(),y_train.to_list(),self.model_name,self.max_seq_length,self.label_map)\n",
        "    \n",
        "      \n",
        "    return train_dataset\n",
        "  def data_prepare_BERT_test(self,X_test):\n",
        "\n",
        "    test_dataset = BERTDatasetTest(X_test.to_list(),self.model_name,self.max_seq_length,self.label_map)\n",
        "    \n",
        "    return test_dataset\n",
        "  def eval(self,X_eval,y_eval, data_set='DEV'):\n",
        "\n",
        "    \"\"\"Evaluate the trained model on a given data set.\n",
        "        Args:\n",
        "            X_eval (:obj:`np array or pandas series`, optional): loaded data for evaluation.\n",
        "\n",
        "            y_eval (:obj:`np array or pandas series`, optional): loaded labels for evaluation.\n",
        "\n",
        "            data_set (:obj:`str`, optional): Name of the provided data set to\n",
        "                use. This is ignored if data_path is not None. Can be either\n",
        "                'VALIDATION' or 'TEST'. Defaults to 'VALIDATION'.\n",
        "        Returns:\n",
        "            :obj:`dict`: A dictionary mapping an evaluation metric to its\n",
        "            computed value. The metrics used are accuracy, f1_micro, f1_macro,\n",
        "            recall_micro, recall_macro, precision_micro and precision_macro.\n",
        "        \"\"\"\n",
        "    validation_inputs = self.data_prepare_BERT(X_eval,y_eval)\n",
        "    predictions=self.trainer.predict(validation_inputs)\n",
        "    all_pred=np.argmax(predictions[0],axis=1)\n",
        "    all_label= [self.label_map[i] for i in y_eval]    \n",
        "    accuracy = accuracy_score(all_label, all_pred)\n",
        "    macro_f1_pos_neg = f1_score(all_label, all_pred,average='macro',labels=[0,1])\n",
        "    f1score = f1_score(all_label, all_pred, average='macro') \n",
        "    recall = recall_score(all_label, all_pred, average='macro')\n",
        "    precision = precision_score(all_label, all_pred, average='macro')\n",
        "    # Get scores\n",
        "    scores = {\n",
        "        'Sentiment': {\n",
        "            'accuracy': accuracy,\n",
        "            'f1_macro': f1score,\n",
        "            'recall_macro': recall,\n",
        "            'precision_macro':precision\n",
        "        }\n",
        "    }\n",
        "    return scores\n",
        "\n",
        "  def predict(self,sentences):\n",
        "    \"\"\"Predict the sentiment  probability scores for a given list of\n",
        "        sentences.\n",
        "        Args:\n",
        "            sentences (:obj:`list` of :obj:`str`): The list of sentences.\n",
        "            output (:obj:`str`): The output label type. Possible values are\n",
        "                'postive', 'neagtive', 'neutral'.\n",
        "        Returns:\n",
        "            :obj:`list` of :obj:`SentIDPred`: A list of prediction results,\n",
        "            each corresponding to its respective sentence.\n",
        "        \"\"\"\n",
        "    if isinstance(sentences, str):\n",
        "      sentences=pd.Series(sentences)\n",
        "    validation_inputs = self.data_prepare_BERT_test(sentences)\n",
        "    predictions=self.trainer.predict(validation_inputs)\n",
        "    probabilities=predictions[0]\n",
        "    predicted = np.argmax(predictions[0],axis=1)   \n",
        "    result = collections.deque()\n",
        "    convert = lambda x: x     \n",
        "    for i in range(0,len(predicted)):\n",
        "      for j, val in enumerate(self.label_map):\n",
        "      \n",
        "        if j==predicted[i]:\n",
        "          result.append(convert(SentIDPred(val, probabilities[i])))\n",
        "          break\n",
        "\n",
        "        \n",
        "      \n",
        "    return list(result)\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rnbIM_hYN1a"
      },
      "source": [
        "# Testing Production code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoqGSsUzYNUI"
      },
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/Omdena_sentiment/Dataset/final_text.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QxVgIOPYdJr"
      },
      "source": [
        "msk = np.random.rand(len(df)) < 0.7\n",
        "train = df[msk]\n",
        "test = df[~msk]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Donm5w_VYe0p"
      },
      "source": [
        "msk = np.random.rand(len(train)) < 0.8\n",
        "train_new = train[msk]\n",
        "valid = train[~msk]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJFtZwWsYg3d"
      },
      "source": [
        "labels_numeric=[0,1,2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNEqmzbUYivv"
      },
      "source": [
        "train_new.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wfbdf1VYkVT"
      },
      "source": [
        "test.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e7vZzyIYlha"
      },
      "source": [
        "valid.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdtYDdeLYnFR"
      },
      "source": [
        "Qarib_Sentiment_Classifier=SentimentIdentification(labels_numeric,num_epoch=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOq6WKPhZJVn"
      },
      "source": [
        "Qarib_Sentiment_Classifier.fine_tune(train_new['final'],train_new['label'],valid['final'],valid['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdJu69QgdmG2"
      },
      "source": [
        "MODEL_PATH_='/content/drive/MyDrive/Omdena_sentiment/Saved_models/Production/QARIB'\n",
        "LABEL_2_INDEX_PATH='/content/drive/MyDrive/Omdena_sentiment/Saved_models/Production/QARIB/_labels_dict.json'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlBNNEMRkySB"
      },
      "source": [
        "Qarib_Sentiment_Classifier.save_model(MODEL_PATH_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_HD_EB0v6j2"
      },
      "source": [
        "Qarib_Sentiment_Classifier.save_label2ind_file(LABEL_2_INDEX_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLHeA_gWRexV"
      },
      "source": [
        "Qarib_Sentiment_Classifier.eval(test['final'],test['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jf3Qlg4CYcU"
      },
      "source": [
        "Qarib_Sentiment_Classifier.predict('صفاء الهاشم سيده كويتيه المراه الوحيده حاليا مجلس الامه الكويتي مدافعه شرسه حقوق المراه وحق المواطن الكويتي وتمتلك عقليه اقتصاديه مميزه اختيرت ضمن سيده عربيه مؤsثره مجتمعها')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXJGuqCBbj9b"
      },
      "source": [
        "Qarib_predictor=SentimentIdentificationPrediction(labels=labels_numeric,training_model_path='/content/drive/MyDrive/Omdena_sentiment/Saved_models/Arabert_production/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2co8dizqbNjz"
      },
      "source": [
        "Qarib_predictor.predict('صفاء الهاشم سيده كويتيه المراه الوحيده حاليا مجلس الامه الكويتي مدافعه شرسه حقوق المراه وحق المواطن الكويتي وتمتلك عقليه اقتصاديه مميزه اختيرت ضمن سيده عربيه مؤsثره مجتمعها')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}