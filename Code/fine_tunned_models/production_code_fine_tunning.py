# -*- coding: utf-8 -*-
"""Production_Code_fine_tunning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1opBTJbnxsCRQwWO5hocOaxtDzf7J3j4j
"""

!wget https://huggingface.co/UBC-NLP/MARBERT/resolve/main/MARBERT_pytorch_verison.tar.gz

!tar -xvf MARBERT_pytorch_verison.tar.gz

!pip install GPUtil pytorch_pretrained_bert==0.5.0 transformers==4.3.0

# (1)load libraries 
import json, sys, regex
import torch
import GPUtil
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from pytorch_pretrained_bert import BertTokenizer, BertConfig, BertAdam, BertForSequenceClassification
from tqdm import tqdm, trange
import pandas as pd
import os
import numpy as np
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix
##----------------------------------------------------
from transformers import *
from transformers import XLMRobertaConfig
from transformers import XLMRobertaModel
from transformers import AutoTokenizer, AutoModelWithLMHead
from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, XLMRobertaModel
from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors
from transformers import AdamW, get_linear_schedule_with_warmup
from transformers import AutoTokenizer, AutoModel
##------------------------------------------------------
import re

MODEL_PATH_BEGIN_FINETUNE='/content/MARBERT_pytorch_verison'

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print ("your device ", device)

class SentimentIdentificationMARBERT(object):
  """A class for finetunning, evaluating and running the sentiment classification on MARBERT model
     After initializing an instance, you must
    run the train method once before using it.
    Args:
        labels (:obj:`set` of :obj:`str`, optional): The set of sentiment labels
            used in the training data in the main model.
            If None, the default labels are used.
            Defaults to None.

        max_seq_length (:obj:`int`, optional): maximum sequence length for the model
            
            If None, the default max_seq_length are used.
            Defaults to 256 .

        num_epoch (:obj:`int`, optional): number of epoch used for training the model
            
            If None, the default num_epoch are used.
            Defaults to 3 .

        batch_size (:obj:`int`, optional):batch size used for training the model            
            If None, the default batch_size are used.
            Defaults to 16 .

        lr (:obj:`int`, optional):initial learning rate used for training the model            
            If None, the default lr are used.
            Defaults to 2e-06 .
        
       
    """
  def __init__(self, labels=None,max_seq_length=256,num_epoch=3,batch_size=16,lr=2e-06
                 ):
        if labels is None:
            self.labels = _DEFAULT_LABELS
        self._labels_sorted = sorted(labels)
        self._is_trained = False
        self.tokenizer = BertTokenizer.from_pretrained(MODEL_PATH_BEGIN_FINETUNE)
        self.num_epoch=num_epoch
        self.batch_size=batch_size
        self.max_seq_length=max_seq_length
        self.lr=lr
  def create_label2ind_file(self):
      self.labels_json = {}
      # convert labels to indexes
      for idx in range(0, len(self._labels_sorted)):
        self.labels_json[self._labels_sorted[idx]] = idx
  def save_label2ind_file(self,path):
    """Save  the label 2 indexr on a given data set.
      Args:
          Path (:obj:`str`): Path where you want to save the feature vector .
              
          
      """
    with open(path, 'w') as json_file:
        json.dump(self.labels_json, json_file)

  def data_prepare_BERT(self,X_train,y_train):
      # Create sentence and label lists
      sentences = X_train.values
      sentences = ['[CLS] ' + sentence + ' [SEP]' for sentence in
                 sentences]
      # Create sentence and label lists
      labels = y_train.values
      labels = [self.labels_json[i] for i in labels]
      # Import the BERT tokenizer, used to convert our text into tokens that correspond to BERT's vocabulary.
      tokenized_texts = [self.tokenizer.tokenize(sent) for sent in sentences]
      # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary
      input_ids = [self.tokenizer.convert_tokens_to_ids(x) for x in
                 tokenized_texts]
      # Pad our input seqeunce to the fixed length (i.e., max_len) with index of [PAD] token
      # ~ input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype="long", truncating="post", padding="post")
      pad_ind = self.tokenizer.convert_tokens_to_ids(['[PAD]'])[0]
      input_ids = pad_sequences(
          input_ids,
          maxlen=self.max_seq_length + 2,
          dtype='long',
          truncating='post',
          padding='post',
          value=pad_ind,
          )
      # Create attention masks
      attention_masks = []
      # Create a mask of 1s for each token followed by 0s for padding
      for seq in input_ids:
        seq_mask = [float(i > 0) for i in seq]
        attention_masks.append(seq_mask)
      # Convert all of our data into torch tensors, the required datatype for our model
      inputs = torch.tensor(input_ids)
      labels = torch.tensor(labels)
      masks = torch.tensor(attention_masks)
      return inputs, labels, masks
  def data_prepare_BERT_test(self,X_train):
      # Create sentence and label lists
      sentences = X_train.values
      sentences = ['[CLS] ' + sentence + ' [SEP]' for sentence in
                 sentences]
     
      # Import the BERT tokenizer, used to convert our text into tokens that correspond to BERT's vocabulary.
      tokenized_texts = [self.tokenizer.tokenize(sent) for sent in sentences]
      # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary
      input_ids = [self.tokenizer.convert_tokens_to_ids(x) for x in
                 tokenized_texts]
      # Pad our input seqeunce to the fixed length (i.e., max_len) with index of [PAD] token
      # ~ input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype="long", truncating="post", padding="post")
      pad_ind = self.tokenizer.convert_tokens_to_ids(['[PAD]'])[0]
      input_ids = pad_sequences(
          input_ids,
          maxlen=self.max_seq_length + 2,
          dtype='long',
          truncating='post',
          padding='post',
          value=pad_ind,
          )
      # Create attention masks
      attention_masks = []
      # Create a mask of 1s for each token followed by 0s for padding
      for seq in input_ids:
        seq_mask = [float(i > 0) for i in seq]
        attention_masks.append(seq_mask)
      # Convert all of our data into torch tensors, the required datatype for our model
      inputs = torch.tensor(input_ids)
      masks = torch.tensor(attention_masks)
      return inputs,  masks
  def train(self,train_dataloader, optimizer, scheduler, criterion,max_grad_norm):
    self.model.train()
    epoch_loss = 0
    for i, batch in enumerate(train_dataloader):
      # Add batch to GPU
      batch = tuple(t.to(device) for t in batch)
      # Unpack the inputs from our dataloader
      input_ids, input_mask, labels = batch
      outputs = self.model(input_ids, input_mask, labels=labels)
      loss, logits = outputs[:2]
      # delete used variables to free GPU memory
      del batch, input_ids, input_mask, labels
      optimizer.zero_grad()
      if torch.cuda.device_count() == 1:
        loss.backward()
        epoch_loss += loss.cpu().item()
      else:

        loss.sum().backward()
         
        epoch_loss += loss.sum().cpu().item()
      optimizer.step()
      torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm)  # Gradient clipping is not in AdamW anymore
      scheduler.step()
    # free GPU memory
    if device == 'cuda':
      torch.cuda.empty_cache()
    return epoch_loss / len(train_dataloader)
  def save_model(self,path):   
    """Save  the model on a given data set.
        Args:
            Path (:obj:`str`): Path where you want to save the model.
               
           
        """
    self.model.save_pretrained(path + '/')

  def eval(self,X_eval,y_eval, data_set='DEV'):

    """Evaluate the trained model on a given data set.
        Args:
            X_eval (:obj:`np array or pandas series`, optional): loaded data for evaluation.

            y_eval (:obj:`np array or pandas series`, optional): loaded labels for evaluation.

            data_set (:obj:`str`, optional): Name of the provided data set to
                use. This is ignored if data_path is not None. Can be either
                'VALIDATION' or 'TEST'. Defaults to 'VALIDATION'.

            batch_size (:obj:`int`, optional):batch size used for training the model            
            If None, the default batch_size are used.
            Defaults to 16 .
        Returns:
            :obj:`dict`: A dictionary mapping an evaluation metric to its
            computed value. The metrics used are accuracy, f1_micro, f1_macro,
            recall_micro, recall_macro, precision_micro and precision_macro.
        """
    validation_inputs, validation_labels, validation_masks = self.data_prepare_BERT(X_eval,y_eval)
    validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)
    validation_dataloader = DataLoader(validation_data, batch_size=self.batch_size)
    self.model.eval()
    all_pred=[]
    all_label = []
    with torch.no_grad():
      for i, batch in enumerate(validation_dataloader):
        # Add batch to GPU
        batch = tuple(t.to(device) for t in batch)
        # Unpack the inputs from our dataloader
        input_ids, input_mask, labels = batch
        outputs = self.model(input_ids, input_mask, labels=labels)
        loss, logits = outputs[:2]
        
        probabilities, predicted = torch.max(logits.cpu().data, 1)
        # put all the true labels and predictions to two lists
        all_pred.extend(predicted)
        all_label.extend(labels.cpu())
    accuracy = accuracy_score(all_label, all_pred)
    macro_f1_pos_neg = f1_score(all_label, all_pred,average='macro',labels=[0,1])
    f1score = f1_score(all_label, all_pred, average='macro') 
    recall = recall_score(all_label, all_pred, average='macro')
    precision = precision_score(all_label, all_pred, average='macro')
    # Get scores
    scores = {
        'Sentiment': {
            'accuracy': accuracy,
            'f1_macro': f1score,
            'recall_macro': recall,
            'precision_macro':precision
        }
    }
    return scores

  def predict(self,sentences):
    """Predict the sentiment  probability scores for a given list of
        sentences.
        Args:
            sentences (:obj:`list` of :obj:`str`): The list of sentences.
            output (:obj:`str`): The output label type. Possible values are
                'postive', 'neagtive', 'neutral'.
        Returns:
            :obj:`list` of :obj:`DIDPred`: A list of prediction results,
            each corresponding to its respective sentence.
        """
    if isinstance(sentences, str):
      sentences=pd.Series(sentences)
    validation_inputs, validation_masks = self.data_prepare_BERT_test(sentences)
    validation_data = TensorDataset(validation_inputs, validation_masks)
    validation_dataloader = DataLoader(validation_data, batch_size=self.batch_size)
    self.model.eval()
    all_pred=[]
    result = collections.deque()
    convert = lambda x: x
    
    with torch.no_grad():
      for i, batch in enumerate(validation_dataloader):
        # Add batch to GPU
        batch = tuple(t.to(device) for t in batch)
        # Unpack the inputs from our dataloader
        input_ids, input_mask = batch
        outputs = self.model(input_ids.to(device), input_mask.to(device))
        logits = outputs[:2][0]
        
        
        
        probabilities, predicted = torch.max(outputs[0].cpu().data, 1)
        for i, val in enumerate(self.label2index):
          if i==predicted.item():
            all_pred.extend((val,probabilities.item()))
            result.append(convert(DIDPred(val, logits.cpu().data.numpy().tolist())))
            break
        # put all the true labels and predictions to two lists

        
      
    return list(result)
      

        
  def evaluate(self,validation_dataloader, criterion):
    
    self.model.eval()
    epoch_loss = 0
    all_pred=[]
    all_label = []
    with torch.no_grad():
      for i, batch in enumerate(validation_dataloader):
        # Add batch to GPU
        batch = tuple(t.to(device) for t in batch)
        # Unpack the inputs from our dataloader
        input_ids, input_mask, labels = batch
        outputs = self.model(input_ids, input_mask, labels=labels)
        loss, logits = outputs[:2]
        # delete used variables to free GPU memory
        del batch, input_ids, input_mask
        if torch.cuda.device_count() == 1:
          epoch_loss += loss.cpu().item()
        else:
          epoch_loss += loss.sum().cpu().item()
        # identify the predicted class for each example in the batch
        probabilities, predicted = torch.max(logits.cpu().data, 1)
        # put all the true labels and predictions to two lists
        all_pred.extend(predicted)
        all_label.extend(labels.cpu())
    accuracy = accuracy_score(all_label, all_pred)
    f1score = f1_score(all_label, all_pred, average='macro')
    recall = recall_score(all_label, all_pred, average='macro')
    precision = precision_score(all_label, all_pred, average='macro')
    report = classification_report(all_label, all_pred)
    print('Accuracy', accuracy)
    print('f1score', f1score)
    print('recall', recall)
    print('precision', precision)
    print('report', report)
    return (epoch_loss / len(validation_dataloader)), accuracy, f1score, recall, precision

         

  def fine_tune(self,X_train,y_train,X_valid=None,y_valid=None):

    """ fine tune MARBERT model.
        Args:
            X_train (:obj:`np array or pandas series`, optional): loaded training data.
               
            y_train (:obj:`np array or pandas series`, optional): loaded labels for training.

            X_valid (:obj:`np array or pandas series`, optional): loaded validation data.
               
            y_valid (:obj:`np array or pandas series`, optional): loaded labels for validation.
       
        """

    self.create_label2ind_file()
    if X_valid is None and y_valid is None:
      msk = np.random.rand(len(X_train)) < 0.8
      X_valid = X_train[~msk]
      X_train = X_train[msk]
      y_valid = y_train[~msk]
      y_train = y_train[msk]
    #-------------------------------------------------------
    train_inputs, train_labels, train_masks = self.data_prepare_BERT(X_train,y_train)
    validation_inputs, validation_labels, validation_masks = self.data_prepare_BERT(X_valid,y_valid)
    #-------------------------------------------------------
    # Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top.
    self.model = BertForSequenceClassification.from_pretrained(MODEL_PATH_BEGIN_FINETUNE, num_labels=len(self.labels_json))
    #-------------------------------------------------------
    train_data = TensorDataset(train_inputs, train_masks, train_labels)
    train_dataloader = DataLoader(train_data, batch_size=self.batch_size)
    #-------------------------------------------------------
    validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)
    validation_dataloader = DataLoader(validation_data, batch_size=self.batch_size)
    #------------------------------------------
    if torch.cuda.is_available():
      if torch.cuda.device_count() == 1:
        print("Run", "with one GPU")
        self.model = self.model.to(device)
      else:
        n_gpu = torch.cuda.device_count()
        print("Run", "with", n_gpu, "GPUs with max 4 GPUs")
        device_ids = GPUtil.getAvailable(limit = 4)
        torch.backends.cudnn.benchmark = True
        self.model = self.model.to(device)
        self.model = nn.DataParallel(model, device_ids=device_ids)
    else:

      print("Run", "with CPU")
      self.model = self.model
    #---------------------------------------------------
    max_grad_norm = 1.0
    warmup_proportion = 0.1
    num_training_steps	= len(train_dataloader) * self.num_epoch
    num_warmup_steps = num_training_steps * warmup_proportion
    optimizer = AdamW(self.model.parameters(), lr=self.lr, correct_bias=False)
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)
    criterion = nn.CrossEntropyLoss()
    for epoch in trange(self.num_epoch, desc="Epoch"):
      train_loss = self.train( train_dataloader, optimizer, scheduler, criterion,max_grad_norm)
      val_loss, val_acc, val_f1, val_recall, val_precision = self.evaluate(validation_dataloader, criterion)

MODEL_PATH_='/content/drive/MyDrive/Omdena_sentiment/Saved_models/Production/MARBERT'
LABEL_2_INDEX_PATH='/content/drive/MyDrive/Omdena_sentiment/Saved_models/Production/MARBERT/_labels-dict.json'

import collections
class DIDPred(collections.namedtuple('SentimentPred', ['top', 'scores'])):
    """A named tuple containing sentiment ID prediction results.
    Attributes:
        top (:obj:`str`): The sentiment label with the highest score. See
            :ref:`sentimentid_labels` for a list of output labels.
        scores (:obj:`dict`): A dictionary mapping each sentiment label to it's
            computed score.
    """

class SentimentIdentificationMARBERTPrediction(object):
  """A class for running a fine-tuned sentiment analysis model to predict
    the sentiment of given sentences.


    Args:
        labels (:obj:`set` of :obj:`str`, optional): The set of dialect labels
            used in the training data in the main model.
            If None, the default labels are used.
            Defaults to None.

        training_model_path (:obj:`str`, optional): Path of training model to be used for inference,
        If none, use defult model for this libaray

        label2index (:obj:`str`, optional): Path of label 2 indexx file to be used for scoring,
        If none, use defult model for this libaray

        max_seq_length (:obj:`int`, optional): maximum sequence length for the model
            
            If None, the default max_seq_length are used.
            Defaults to 256 .

    """
  def __init__(self, labels=None,training_model_path=None,label2index=None,max_seq_length=256,
                batch_size=16 ):
        if labels is None:
            self.labels = _DEFAULT_LABELS
        self._labels_sorted = sorted(labels)
        self.tokenizer = BertTokenizer.from_pretrained(MODEL_PATH_BEGIN_FINETUNE)
        self.batch_size=batch_size
        self.max_seq_length=max_seq_length
        if label2index is None:
          self.label2index= json.load(open(LABEL_2_INDEX_PATH))
        else:
          self.label2index=  json.load(open(label2index))

        if training_model_path is None:
          self.model=BertForSequenceClassification.from_pretrained(MODEL_PATH_, num_labels=len(self.label2index))
          
        else:
          self.model=BertForSequenceClassification.from_pretrained(training_model_path, num_labels=len(self.label2index))

        if torch.cuda.is_available():
          if torch.cuda.device_count() == 1:
            print("Run", "with one GPU")
            self.model = self.model.to(device)
          else:
            n_gpu = torch.cuda.device_count()
            print("Run", "with", n_gpu, "GPUs with max 4 GPUs")
            device_ids = GPUtil.getAvailable(limit = 4)
            torch.backends.cudnn.benchmark = True
            self.model = self.model.to(device)
            self.model = nn.DataParallel(self.model, device_ids=device_ids)
        else:
          print("Run", "with CPU")
          self.model = self.model
        
  def data_prepare_BERT(self,X_train,y_train):
      # Create sentence and label lists
      sentences = X_train.values
      sentences = ['[CLS] ' + sentence + ' [SEP]' for sentence in
                 sentences]
      # Create sentence and label lists
      labels = y_train.values
      labels = [self.labels_json[i] for i in labels]
      # Import the BERT tokenizer, used to convert our text into tokens that correspond to BERT's vocabulary.
      tokenized_texts = [self.tokenizer.tokenize(sent) for sent in sentences]
      # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary
      input_ids = [self.tokenizer.convert_tokens_to_ids(x) for x in
                 tokenized_texts]
      # Pad our input seqeunce to the fixed length (i.e., max_len) with index of [PAD] token
      # ~ input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype="long", truncating="post", padding="post")
      pad_ind = self.tokenizer.convert_tokens_to_ids(['[PAD]'])[0]
      input_ids = pad_sequences(
          input_ids,
          maxlen=self.max_seq_length + 2,
          dtype='long',
          truncating='post',
          padding='post',
          value=pad_ind,
          )
      # Create attention masks
      attention_masks = []
      # Create a mask of 1s for each token followed by 0s for padding
      for seq in input_ids:
        seq_mask = [float(i > 0) for i in seq]
        attention_masks.append(seq_mask)
      # Convert all of our data into torch tensors, the required datatype for our model
      inputs = torch.tensor(input_ids)
      labels = torch.tensor(labels)
      masks = torch.tensor(attention_masks)
      return inputs, labels, masks
  def data_prepare_BERT_test(self,X_train):
      # Create sentence and label lists
      sentences = X_train.values
      sentences = ['[CLS] ' + sentence + ' [SEP]' for sentence in
                 sentences]
     
      # Import the BERT tokenizer, used to convert our text into tokens that correspond to BERT's vocabulary.
      tokenized_texts = [self.tokenizer.tokenize(sent) for sent in sentences]
      # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary
      input_ids = [self.tokenizer.convert_tokens_to_ids(x) for x in
                 tokenized_texts]
      # Pad our input seqeunce to the fixed length (i.e., max_len) with index of [PAD] token
      # ~ input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype="long", truncating="post", padding="post")
      pad_ind = self.tokenizer.convert_tokens_to_ids(['[PAD]'])[0]
      input_ids = pad_sequences(
          input_ids,
          maxlen=self.max_seq_length + 2,
          dtype='long',
          truncating='post',
          padding='post',
          value=pad_ind,
          )
      # Create attention masks
      attention_masks = []
      # Create a mask of 1s for each token followed by 0s for padding
      for seq in input_ids:
        seq_mask = [float(i > 0) for i in seq]
        attention_masks.append(seq_mask)
      # Convert all of our data into torch tensors, the required datatype for our model
      inputs = torch.tensor(input_ids)
      masks = torch.tensor(attention_masks)
      return inputs,  masks
  def eval(self,X_eval,y_eval, data_set='DEV'):

    """Evaluate the trained model on a given data set.
        Args:
            X_eval (:obj:`np array or pandas series`, optional): loaded data for evaluation.

            y_eval (:obj:`np array or pandas series`, optional): loaded labels for evaluation.

            data_set (:obj:`str`, optional): Name of the provided data set to
                use. This is ignored if data_path is not None. Can be either
                'VALIDATION' or 'TEST'. Defaults to 'VALIDATION'.
        Returns:
            :obj:`dict`: A dictionary mapping an evaluation metric to its
            computed value. The metrics used are accuracy, f1_micro, f1_macro,
            recall_micro, recall_macro, precision_micro and precision_macro.
        """
    validation_inputs, validation_labels, validation_masks = self.data_prepare_BERT(X_eval,y_eval)
    validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)
    validation_dataloader = DataLoader(validation_data, batch_size=self.batch_size)
    self.model.eval()
    all_pred=[]
    all_label = []
    with torch.no_grad():
      for i, batch in enumerate(validation_dataloader):
        # Add batch to GPU
        batch = tuple(t.to(device) for t in batch)
        # Unpack the inputs from our dataloader
        input_ids, input_mask, labels = batch
        outputs = self.model(input_ids, input_mask, labels=labels)
        loss, logits = outputs[:2]
        
        probabilities, predicted = torch.max(logits.cpu().data, 1)
        # put all the true labels and predictions to two lists
        all_pred.extend(predicted)
        all_label.extend(labels.cpu())
    accuracy = accuracy_score(all_label, all_pred)
    macro_f1_pos_neg = f1_score(all_label, all_pred,average='macro',labels=[0,1])
    f1score = f1_score(all_label, all_pred, average='macro') 
    recall = recall_score(all_label, all_pred, average='macro')
    precision = precision_score(all_label, all_pred, average='macro')
    # Get scores
    scores = {
        'Sentiment': {
            'accuracy': accuracy,
            'f1_macro': f1score,
            'recall_macro': recall,
            'precision_macro':precision
        }
    }
    return scores

  def predict(self,sentences):
    """Predict the sentiment  probability scores for a given list of
        sentences.
        Args:
            sentences (:obj:`list` of :obj:`str`): The list of sentences.
            output (:obj:`str`): The output label type. Possible values are
                'postive', 'neagtive', 'neutral'.
        Returns:
            :obj:`list` of :obj:`DIDPred`: A list of prediction results,
            each corresponding to its respective sentence.
        """
    if isinstance(sentences, str):
      sentences=pd.Series(sentences)
    validation_inputs, validation_masks = self.data_prepare_BERT_test(sentences)
    validation_data = TensorDataset(validation_inputs, validation_masks)
    validation_dataloader = DataLoader(validation_data, batch_size=self.batch_size)
    self.model.eval()
    all_pred=[]
    result = collections.deque()
    convert = lambda x: x
    
    with torch.no_grad():
      for i, batch in enumerate(validation_dataloader):
        # Add batch to GPU
        batch = tuple(t.to(device) for t in batch)
        # Unpack the inputs from our dataloader
        input_ids, input_mask = batch
        outputs = self.model(input_ids.to(device), input_mask.to(device))
        logits = outputs[:2][0]
        
        
        
        probabilities, predicted = torch.max(outputs[0].cpu().data, 1)
        for i, val in enumerate(self.label2index):
          if i==predicted.item():
            all_pred.extend((val,probabilities.item()))
            result.append(convert(DIDPred(val, logits.cpu().data.numpy().tolist())))
            break
        # put all the true labels and predictions to two lists

        
      
    return list(result)

"""# Testing Production code"""

df=pd.read_csv('/content/drive/MyDrive/Omdena_sentiment/Dataset/final_text.csv')

msk = np.random.rand(len(df)) < 0.7
train = df[msk]
test = df[~msk]

msk = np.random.rand(len(train)) < 0.8
train_new = train[msk]
valid = train[~msk]

labels_numeric=[0,1,2]

train_new.dropna(inplace=True)

test.dropna(inplace=True)

valid.dropna(inplace=True)

MARBERT_Sentiment_Classifier=SentimentIdentificationMARBERT(labels_numeric,num_epoch=1)

MARBERT_Sentiment_Classifier.fine_tune(train_new['final'],train_new['label'],valid['final'],valid['label'])

MARBERT_Sentiment_Classifier.save_model('/content/drive/MyDrive/Omdena_sentiment/Saved_models/Production/MARBERT')

MARBERT_Sentiment_Classifier.save_label2ind_file('/content/drive/MyDrive/Omdena_sentiment/Saved_models/Production/MARBERT/_labels-dict.json')

MARBERT_Sentiment_Classifier.eval(test['final'],test['label'])

Marbert_predictor=SentimentIdentificationMARBERTPrediction(labels_numeric)

Marbert_predictor.predict('صفاء الهاشم سيده كويتيه المراه الوحيده حاليا مجلس الامه الكويتي مدافعه شرسه حقوق المراه وحق المواطن الكويتي وتمتلك عقليه اقتصاديه مميزه اختيرت ضمن سيده عربيه مؤsثره مجتمعها')