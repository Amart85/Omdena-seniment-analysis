{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Production_Code_fine_tunning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgx39ygQ3cUE",
        "outputId": "2e4250c0-0c57-437a-e59c-7ce65c93164f"
      },
      "source": [
        "!wget https://huggingface.co/UBC-NLP/MARBERT/resolve/main/MARBERT_pytorch_verison.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-09 15:24:39--  https://huggingface.co/UBC-NLP/MARBERT/resolve/main/MARBERT_pytorch_verison.tar.gz\n",
            "Resolving huggingface.co (huggingface.co)... 15.197.130.34\n",
            "Connecting to huggingface.co (huggingface.co)|15.197.130.34|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/UBC-NLP/MARBERT/85bfec76f38cba4bc2e6cd02a959016de37ba93de4c850a7d175811dce4e8adc [following]\n",
            "--2021-06-09 15:24:40--  https://cdn-lfs.huggingface.co/UBC-NLP/MARBERT/85bfec76f38cba4bc2e6cd02a959016de37ba93de4c850a7d175811dce4e8adc\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 99.86.113.22, 99.86.113.119, 99.86.113.117, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|99.86.113.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 607066087 (579M) [application/x-gzip]\n",
            "Saving to: ‘MARBERT_pytorch_verison.tar.gz’\n",
            "\n",
            "MARBERT_pytorch_ver 100%[===================>] 578.94M   143MB/s    in 4.1s    \n",
            "\n",
            "2021-06-09 15:24:44 (143 MB/s) - ‘MARBERT_pytorch_verison.tar.gz’ saved [607066087/607066087]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y22ebgwt3wtk",
        "outputId": "f64c2b85-2901-4156-fa0c-abffbba7f226"
      },
      "source": [
        "!tar -xvf MARBERT_pytorch_verison.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MARBERT_pytorch_verison/\n",
            "MARBERT_pytorch_verison/pytorch_model.bin\n",
            "MARBERT_pytorch_verison/config.json\n",
            "MARBERT_pytorch_verison/vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmaNVL8R3zbc",
        "outputId": "e470bf4e-d793-4c02-941c-5f7841431d6c"
      },
      "source": [
        "!pip install GPUtil pytorch_pretrained_bert==0.5.0 transformers==4.3.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting GPUtil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Collecting pytorch_pretrained_bert==0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/f6/bd094fb2cf6a3232fcb79bc4067e5322218ecc26170b4db94976fa805e30/pytorch_pretrained_bert-0.5.0-py3-none-any.whl (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 6.7MB/s \n",
            "\u001b[?25hCollecting transformers==4.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/bb/61bf4a221150aff2756935d032a83f2e68976b050e8606d990a0f3054179/transformers-4.3.0-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 23.0MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/e1/1b164502f455035def771ec7a31f705351b7f953695d57ce26219aaf21a9/boto3-1.17.90-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 36.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert==0.5.0) (4.41.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert==0.5.0) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert==0.5.0) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert==0.5.0) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 35.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.0) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.0) (4.0.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 35.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.0) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.0) (20.9)\n",
            "Collecting s3transfer<0.5.0,>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.2MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.21.0,>=1.20.90\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/ac/617d3ac25ea905279deb06edd82d6c19ca272006d6dcf232b837b75c3dde/botocore-1.20.90-py2.py3-none-any.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 33.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert==0.5.0) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert==0.5.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert==0.5.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert==0.5.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert==0.5.0) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.0) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.0) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.3.0) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.3.0) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.90->boto3->pytorch_pretrained_bert==0.5.0) (2.8.1)\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-cp37-none-any.whl size=7411 sha256=d048d797f935bc6f060abde9f787f358ee0c1a5e3912158a48707982e06369b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built GPUtil\n",
            "\u001b[31mERROR: botocore 1.20.90 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: GPUtil, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert, sacremoses, tokenizers, transformers\n",
            "Successfully installed GPUtil-1.4.0 boto3-1.17.90 botocore-1.20.90 jmespath-0.10.0 pytorch-pretrained-bert-0.5.0 s3transfer-0.4.2 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IYJ42tQ318w",
        "outputId": "cb0026d5-6a97-4bd6-e614-d203f5726b8a"
      },
      "source": [
        "# (1)load libraries \n",
        "import json, sys, regex\n",
        "import torch\n",
        "import GPUtil\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig, BertAdam, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix\n",
        "##----------------------------------------------------\n",
        "from transformers import *\n",
        "from transformers import XLMRobertaConfig\n",
        "from transformers import XLMRobertaModel\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, XLMRobertaModel\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "##------------------------------------------------------\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsddGpRf61WO"
      },
      "source": [
        "MODEL_PATH_BEGIN_FINETUNE='/content/MARBERT_pytorch_verison'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqtS9ym3c_RL",
        "outputId": "a3b64a78-9239-485b-f209-59cfbfcb3958"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print (\"your device \", device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "your device  cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ewzCNgb3-dS"
      },
      "source": [
        "class SentimentIdentificationMARBERT(object):\n",
        "  \"\"\"A class for finetunning, evaluating and running the sentiment classification on MARBERT model\n",
        "     After initializing an instance, you must\n",
        "    run the train method once before using it.\n",
        "    Args:\n",
        "        labels (:obj:`set` of :obj:`str`, optional): The set of sentiment labels\n",
        "            used in the training data in the main model.\n",
        "            If None, the default labels are used.\n",
        "            Defaults to None.\n",
        "\n",
        "        max_seq_length (:obj:`int`, optional): maximum sequence length for the model\n",
        "            \n",
        "            If None, the default max_seq_length are used.\n",
        "            Defaults to 256 .\n",
        "\n",
        "        num_epoch (:obj:`int`, optional): number of epoch used for training the model\n",
        "            \n",
        "            If None, the default num_epoch are used.\n",
        "            Defaults to 3 .\n",
        "\n",
        "        batch_size (:obj:`int`, optional):batch size used for training the model            \n",
        "            If None, the default batch_size are used.\n",
        "            Defaults to 16 .\n",
        "\n",
        "        lr (:obj:`int`, optional):initial learning rate used for training the model            \n",
        "            If None, the default lr are used.\n",
        "            Defaults to 2e-06 .\n",
        "        \n",
        "       \n",
        "    \"\"\"\n",
        "  def __init__(self, labels=None,max_seq_length=256,num_epoch=3,batch_size=16,lr=2e-06\n",
        "                 ):\n",
        "        if labels is None:\n",
        "            self.labels = _DEFAULT_LABELS\n",
        "        self._labels_sorted = sorted(labels)\n",
        "        self._is_trained = False\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(MODEL_PATH_BEGIN_FINETUNE)\n",
        "        self.num_epoch=num_epoch\n",
        "        self.batch_size=batch_size\n",
        "        self.max_seq_length=max_seq_length\n",
        "        self.lr=lr\n",
        "  def create_label2ind_file(self):\n",
        "      self.labels_json = {}\n",
        "      # convert labels to indexes\n",
        "      for idx in range(0, len(self._labels_sorted)):\n",
        "        self.labels_json[self._labels_sorted[idx]] = idx\n",
        "  def save_label2ind_file(self,path):\n",
        "    \"\"\"Save  the label 2 indexr on a given data set.\n",
        "      Args:\n",
        "          Path (:obj:`str`): Path where you want to save the feature vector .\n",
        "              \n",
        "          \n",
        "      \"\"\"\n",
        "    with open(path, 'w') as json_file:\n",
        "        json.dump(self.labels_json, json_file)\n",
        "\n",
        "  def data_prepare_BERT(self,X_train,y_train):\n",
        "      # Create sentence and label lists\n",
        "      sentences = X_train.values\n",
        "      sentences = ['[CLS] ' + sentence + ' [SEP]' for sentence in\n",
        "                 sentences]\n",
        "      # Create sentence and label lists\n",
        "      labels = y_train.values\n",
        "      labels = [self.labels_json[i] for i in labels]\n",
        "      # Import the BERT tokenizer, used to convert our text into tokens that correspond to BERT's vocabulary.\n",
        "      tokenized_texts = [self.tokenizer.tokenize(sent) for sent in sentences]\n",
        "      # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "      input_ids = [self.tokenizer.convert_tokens_to_ids(x) for x in\n",
        "                 tokenized_texts]\n",
        "      # Pad our input seqeunce to the fixed length (i.e., max_len) with index of [PAD] token\n",
        "      # ~ input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "      pad_ind = self.tokenizer.convert_tokens_to_ids(['[PAD]'])[0]\n",
        "      input_ids = pad_sequences(\n",
        "          input_ids,\n",
        "          maxlen=self.max_seq_length + 2,\n",
        "          dtype='long',\n",
        "          truncating='post',\n",
        "          padding='post',\n",
        "          value=pad_ind,\n",
        "          )\n",
        "      # Create attention masks\n",
        "      attention_masks = []\n",
        "      # Create a mask of 1s for each token followed by 0s for padding\n",
        "      for seq in input_ids:\n",
        "        seq_mask = [float(i > 0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "      # Convert all of our data into torch tensors, the required datatype for our model\n",
        "      inputs = torch.tensor(input_ids)\n",
        "      labels = torch.tensor(labels)\n",
        "      masks = torch.tensor(attention_masks)\n",
        "      return inputs, labels, masks\n",
        "  def data_prepare_BERT_test(self,X_train):\n",
        "      # Create sentence and label lists\n",
        "      sentences = X_train.values\n",
        "      sentences = ['[CLS] ' + sentence + ' [SEP]' for sentence in\n",
        "                 sentences]\n",
        "     \n",
        "      # Import the BERT tokenizer, used to convert our text into tokens that correspond to BERT's vocabulary.\n",
        "      tokenized_texts = [self.tokenizer.tokenize(sent) for sent in sentences]\n",
        "      # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "      input_ids = [self.tokenizer.convert_tokens_to_ids(x) for x in\n",
        "                 tokenized_texts]\n",
        "      # Pad our input seqeunce to the fixed length (i.e., max_len) with index of [PAD] token\n",
        "      # ~ input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "      pad_ind = self.tokenizer.convert_tokens_to_ids(['[PAD]'])[0]\n",
        "      input_ids = pad_sequences(\n",
        "          input_ids,\n",
        "          maxlen=self.max_seq_length + 2,\n",
        "          dtype='long',\n",
        "          truncating='post',\n",
        "          padding='post',\n",
        "          value=pad_ind,\n",
        "          )\n",
        "      # Create attention masks\n",
        "      attention_masks = []\n",
        "      # Create a mask of 1s for each token followed by 0s for padding\n",
        "      for seq in input_ids:\n",
        "        seq_mask = [float(i > 0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "      # Convert all of our data into torch tensors, the required datatype for our model\n",
        "      inputs = torch.tensor(input_ids)\n",
        "      masks = torch.tensor(attention_masks)\n",
        "      return inputs,  masks\n",
        "  def train(self,train_dataloader, optimizer, scheduler, criterion,max_grad_norm):\n",
        "    self.model.train()\n",
        "    epoch_loss = 0\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "      # Add batch to GPU\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      # Unpack the inputs from our dataloader\n",
        "      input_ids, input_mask, labels = batch\n",
        "      outputs = self.model(input_ids, input_mask, labels=labels)\n",
        "      loss, logits = outputs[:2]\n",
        "      # delete used variables to free GPU memory\n",
        "      del batch, input_ids, input_mask, labels\n",
        "      optimizer.zero_grad()\n",
        "      if torch.cuda.device_count() == 1:\n",
        "        loss.backward()\n",
        "        epoch_loss += loss.cpu().item()\n",
        "      else:\n",
        "\n",
        "        loss.sum().backward()\n",
        "         \n",
        "        epoch_loss += loss.sum().cpu().item()\n",
        "      optimizer.step()\n",
        "      torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm)  # Gradient clipping is not in AdamW anymore\n",
        "      scheduler.step()\n",
        "    # free GPU memory\n",
        "    if device == 'cuda':\n",
        "      torch.cuda.empty_cache()\n",
        "    return epoch_loss / len(train_dataloader)\n",
        "  def save_model(self,path):   \n",
        "    \"\"\"Save  the model on a given data set.\n",
        "        Args:\n",
        "            Path (:obj:`str`): Path where you want to save the model.\n",
        "               \n",
        "           \n",
        "        \"\"\"\n",
        "    self.model.save_pretrained(path + '/')\n",
        "\n",
        "  def eval(self,X_eval,y_eval, data_set='DEV'):\n",
        "\n",
        "    \"\"\"Evaluate the trained model on a given data set.\n",
        "        Args:\n",
        "            X_eval (:obj:`np array or pandas series`, optional): loaded data for evaluation.\n",
        "\n",
        "            y_eval (:obj:`np array or pandas series`, optional): loaded labels for evaluation.\n",
        "\n",
        "            data_set (:obj:`str`, optional): Name of the provided data set to\n",
        "                use. This is ignored if data_path is not None. Can be either\n",
        "                'VALIDATION' or 'TEST'. Defaults to 'VALIDATION'.\n",
        "\n",
        "            batch_size (:obj:`int`, optional):batch size used for training the model            \n",
        "            If None, the default batch_size are used.\n",
        "            Defaults to 16 .\n",
        "        Returns:\n",
        "            :obj:`dict`: A dictionary mapping an evaluation metric to its\n",
        "            computed value. The metrics used are accuracy, f1_micro, f1_macro,\n",
        "            recall_micro, recall_macro, precision_micro and precision_macro.\n",
        "        \"\"\"\n",
        "    validation_inputs, validation_labels, validation_masks = self.data_prepare_BERT(X_eval,y_eval)\n",
        "    validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "    validation_dataloader = DataLoader(validation_data, batch_size=self.batch_size)\n",
        "    self.model.eval()\n",
        "    all_pred=[]\n",
        "    all_label = []\n",
        "    with torch.no_grad():\n",
        "      for i, batch in enumerate(validation_dataloader):\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from our dataloader\n",
        "        input_ids, input_mask, labels = batch\n",
        "        outputs = self.model(input_ids, input_mask, labels=labels)\n",
        "        loss, logits = outputs[:2]\n",
        "        \n",
        "        probabilities, predicted = torch.max(logits.cpu().data, 1)\n",
        "        # put all the true labels and predictions to two lists\n",
        "        all_pred.extend(predicted)\n",
        "        all_label.extend(labels.cpu())\n",
        "    accuracy = accuracy_score(all_label, all_pred)\n",
        "    macro_f1_pos_neg = f1_score(all_label, all_pred,average='macro',labels=[0,1])\n",
        "    f1score = f1_score(all_label, all_pred, average='macro') \n",
        "    recall = recall_score(all_label, all_pred, average='macro')\n",
        "    precision = precision_score(all_label, all_pred, average='macro')\n",
        "    # Get scores\n",
        "    scores = {\n",
        "        'Sentiment': {\n",
        "            'accuracy': accuracy,\n",
        "            'f1_macro': f1score,\n",
        "            'recall_macro': recall,\n",
        "            'precision_macro':precision\n",
        "        }\n",
        "    }\n",
        "    return scores\n",
        "\n",
        "  def predict(self,sentences):\n",
        "    \"\"\"Predict the sentiment  probability scores for a given list of\n",
        "        sentences.\n",
        "        Args:\n",
        "            sentences (:obj:`list` of :obj:`str`): The list of sentences.\n",
        "            output (:obj:`str`): The output label type. Possible values are\n",
        "                'postive', 'neagtive', 'neutral'.\n",
        "        Returns:\n",
        "            :obj:`list` of :obj:`DIDPred`: A list of prediction results,\n",
        "            each corresponding to its respective sentence.\n",
        "        \"\"\"\n",
        "    if isinstance(sentences, str):\n",
        "      sentences=pd.Series(sentences)\n",
        "    validation_inputs, validation_masks = self.data_prepare_BERT_test(sentences)\n",
        "    validation_data = TensorDataset(validation_inputs, validation_masks)\n",
        "    validation_dataloader = DataLoader(validation_data, batch_size=self.batch_size)\n",
        "    self.model.eval()\n",
        "    all_pred=[]\n",
        "    result = collections.deque()\n",
        "    convert = lambda x: x\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      for i, batch in enumerate(validation_dataloader):\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from our dataloader\n",
        "        input_ids, input_mask = batch\n",
        "        outputs = self.model(input_ids.to(device), input_mask.to(device))\n",
        "        logits = outputs[:2][0]\n",
        "        \n",
        "        \n",
        "        \n",
        "        probabilities, predicted = torch.max(outputs[0].cpu().data, 1)\n",
        "        for i, val in enumerate(self.label2index):\n",
        "          if i==predicted.item():\n",
        "            all_pred.extend((val,probabilities.item()))\n",
        "            result.append(convert(DIDPred(val, logits.cpu().data.numpy().tolist())))\n",
        "            break\n",
        "        # put all the true labels and predictions to two lists\n",
        "\n",
        "        \n",
        "      \n",
        "    return list(result)\n",
        "      \n",
        "\n",
        "        \n",
        "  def evaluate(self,validation_dataloader, criterion):\n",
        "    \n",
        "    self.model.eval()\n",
        "    epoch_loss = 0\n",
        "    all_pred=[]\n",
        "    all_label = []\n",
        "    with torch.no_grad():\n",
        "      for i, batch in enumerate(validation_dataloader):\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from our dataloader\n",
        "        input_ids, input_mask, labels = batch\n",
        "        outputs = self.model(input_ids, input_mask, labels=labels)\n",
        "        loss, logits = outputs[:2]\n",
        "        # delete used variables to free GPU memory\n",
        "        del batch, input_ids, input_mask\n",
        "        if torch.cuda.device_count() == 1:\n",
        "          epoch_loss += loss.cpu().item()\n",
        "        else:\n",
        "          epoch_loss += loss.sum().cpu().item()\n",
        "        # identify the predicted class for each example in the batch\n",
        "        probabilities, predicted = torch.max(logits.cpu().data, 1)\n",
        "        # put all the true labels and predictions to two lists\n",
        "        all_pred.extend(predicted)\n",
        "        all_label.extend(labels.cpu())\n",
        "    accuracy = accuracy_score(all_label, all_pred)\n",
        "    f1score = f1_score(all_label, all_pred, average='macro')\n",
        "    recall = recall_score(all_label, all_pred, average='macro')\n",
        "    precision = precision_score(all_label, all_pred, average='macro')\n",
        "    report = classification_report(all_label, all_pred)\n",
        "    print('Accuracy', accuracy)\n",
        "    print('f1score', f1score)\n",
        "    print('recall', recall)\n",
        "    print('precision', precision)\n",
        "    print('report', report)\n",
        "    return (epoch_loss / len(validation_dataloader)), accuracy, f1score, recall, precision\n",
        "\n",
        "         \n",
        "\n",
        "  def fine_tune(self,X_train,y_train,X_valid=None,y_valid=None):\n",
        "\n",
        "    \"\"\" fine tune MARBERT model.\n",
        "        Args:\n",
        "            X_train (:obj:`np array or pandas series`, optional): loaded training data.\n",
        "               \n",
        "            y_train (:obj:`np array or pandas series`, optional): loaded labels for training.\n",
        "\n",
        "            X_valid (:obj:`np array or pandas series`, optional): loaded validation data.\n",
        "               \n",
        "            y_valid (:obj:`np array or pandas series`, optional): loaded labels for validation.\n",
        "       \n",
        "        \"\"\"\n",
        "\n",
        "    self.create_label2ind_file()\n",
        "    if X_valid is None and y_valid is None:\n",
        "      msk = np.random.rand(len(X_train)) < 0.8\n",
        "      X_valid = X_train[~msk]\n",
        "      X_train = X_train[msk]\n",
        "      y_valid = y_train[~msk]\n",
        "      y_train = y_train[msk]\n",
        "    #-------------------------------------------------------\n",
        "    train_inputs, train_labels, train_masks = self.data_prepare_BERT(X_train,y_train)\n",
        "    validation_inputs, validation_labels, validation_masks = self.data_prepare_BERT(X_valid,y_valid)\n",
        "    #-------------------------------------------------------\n",
        "    # Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top.\n",
        "    self.model = BertForSequenceClassification.from_pretrained(MODEL_PATH_BEGIN_FINETUNE, num_labels=len(self.labels_json))\n",
        "    #-------------------------------------------------------\n",
        "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "    train_dataloader = DataLoader(train_data, batch_size=self.batch_size)\n",
        "    #-------------------------------------------------------\n",
        "    validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "    validation_dataloader = DataLoader(validation_data, batch_size=self.batch_size)\n",
        "    #------------------------------------------\n",
        "    if torch.cuda.is_available():\n",
        "      if torch.cuda.device_count() == 1:\n",
        "        print(\"Run\", \"with one GPU\")\n",
        "        self.model = self.model.to(device)\n",
        "      else:\n",
        "        n_gpu = torch.cuda.device_count()\n",
        "        print(\"Run\", \"with\", n_gpu, \"GPUs with max 4 GPUs\")\n",
        "        device_ids = GPUtil.getAvailable(limit = 4)\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        self.model = self.model.to(device)\n",
        "        self.model = nn.DataParallel(model, device_ids=device_ids)\n",
        "    else:\n",
        "\n",
        "      print(\"Run\", \"with CPU\")\n",
        "      self.model = self.model\n",
        "    #---------------------------------------------------\n",
        "    max_grad_norm = 1.0\n",
        "    warmup_proportion = 0.1\n",
        "    num_training_steps\t= len(train_dataloader) * self.num_epoch\n",
        "    num_warmup_steps = num_training_steps * warmup_proportion\n",
        "    optimizer = AdamW(self.model.parameters(), lr=self.lr, correct_bias=False)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for epoch in trange(self.num_epoch, desc=\"Epoch\"):\n",
        "      train_loss = self.train( train_dataloader, optimizer, scheduler, criterion,max_grad_norm)\n",
        "      val_loss, val_acc, val_f1, val_recall, val_precision = self.evaluate(validation_dataloader, criterion)\n",
        "  \n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdJu69QgdmG2"
      },
      "source": [
        "MODEL_PATH_='/content/drive/MyDrive/Omdena_sentiment/Saved_models/Production/MARBERT'\n",
        "LABEL_2_INDEX_PATH='/content/drive/MyDrive/Omdena_sentiment/Saved_models/Production/MARBERT/_labels-dict.json'"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbSE86REiRYt"
      },
      "source": [
        "import collections\n",
        "class DIDPred(collections.namedtuple('SentimentPred', ['top', 'scores'])):\n",
        "    \"\"\"A named tuple containing sentiment ID prediction results.\n",
        "    Attributes:\n",
        "        top (:obj:`str`): The sentiment label with the highest score. See\n",
        "            :ref:`sentimentid_labels` for a list of output labels.\n",
        "        scores (:obj:`dict`): A dictionary mapping each sentiment label to it's\n",
        "            computed score.\n",
        "    \"\"\""
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIqttStecsh7"
      },
      "source": [
        "class SentimentIdentificationMARBERTPrediction(object):\n",
        "  \"\"\"A class for running a fine-tuned sentiment analysis model to predict\n",
        "    the sentiment of given sentences.\n",
        "\n",
        "\n",
        "    Args:\n",
        "        labels (:obj:`set` of :obj:`str`, optional): The set of dialect labels\n",
        "            used in the training data in the main model.\n",
        "            If None, the default labels are used.\n",
        "            Defaults to None.\n",
        "\n",
        "        training_model_path (:obj:`str`, optional): Path of training model to be used for inference,\n",
        "        If none, use defult model for this libaray\n",
        "\n",
        "        label2index (:obj:`str`, optional): Path of label 2 indexx file to be used for scoring,\n",
        "        If none, use defult model for this libaray\n",
        "\n",
        "        max_seq_length (:obj:`int`, optional): maximum sequence length for the model\n",
        "            \n",
        "            If None, the default max_seq_length are used.\n",
        "            Defaults to 256 .\n",
        "\n",
        "    \"\"\"\n",
        "  def __init__(self, labels=None,training_model_path=None,label2index=None,max_seq_length=256,\n",
        "                batch_size=16 ):\n",
        "        if labels is None:\n",
        "            self.labels = _DEFAULT_LABELS\n",
        "        self._labels_sorted = sorted(labels)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(MODEL_PATH_BEGIN_FINETUNE)\n",
        "        self.batch_size=batch_size\n",
        "        self.max_seq_length=max_seq_length\n",
        "        if label2index is None:\n",
        "          self.label2index= json.load(open(LABEL_2_INDEX_PATH))\n",
        "        else:\n",
        "          self.label2index=  json.load(open(label2index))\n",
        "\n",
        "        if training_model_path is None:\n",
        "          self.model=BertForSequenceClassification.from_pretrained(MODEL_PATH_, num_labels=len(self.label2index))\n",
        "          \n",
        "        else:\n",
        "          self.model=BertForSequenceClassification.from_pretrained(training_model_path, num_labels=len(self.label2index))\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "          if torch.cuda.device_count() == 1:\n",
        "            print(\"Run\", \"with one GPU\")\n",
        "            self.model = self.model.to(device)\n",
        "          else:\n",
        "            n_gpu = torch.cuda.device_count()\n",
        "            print(\"Run\", \"with\", n_gpu, \"GPUs with max 4 GPUs\")\n",
        "            device_ids = GPUtil.getAvailable(limit = 4)\n",
        "            torch.backends.cudnn.benchmark = True\n",
        "            self.model = self.model.to(device)\n",
        "            self.model = nn.DataParallel(self.model, device_ids=device_ids)\n",
        "        else:\n",
        "          print(\"Run\", \"with CPU\")\n",
        "          self.model = self.model\n",
        "        \n",
        "  def data_prepare_BERT(self,X_train,y_train):\n",
        "      # Create sentence and label lists\n",
        "      sentences = X_train.values\n",
        "      sentences = ['[CLS] ' + sentence + ' [SEP]' for sentence in\n",
        "                 sentences]\n",
        "      # Create sentence and label lists\n",
        "      labels = y_train.values\n",
        "      labels = [self.labels_json[i] for i in labels]\n",
        "      # Import the BERT tokenizer, used to convert our text into tokens that correspond to BERT's vocabulary.\n",
        "      tokenized_texts = [self.tokenizer.tokenize(sent) for sent in sentences]\n",
        "      # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "      input_ids = [self.tokenizer.convert_tokens_to_ids(x) for x in\n",
        "                 tokenized_texts]\n",
        "      # Pad our input seqeunce to the fixed length (i.e., max_len) with index of [PAD] token\n",
        "      # ~ input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "      pad_ind = self.tokenizer.convert_tokens_to_ids(['[PAD]'])[0]\n",
        "      input_ids = pad_sequences(\n",
        "          input_ids,\n",
        "          maxlen=self.max_seq_length + 2,\n",
        "          dtype='long',\n",
        "          truncating='post',\n",
        "          padding='post',\n",
        "          value=pad_ind,\n",
        "          )\n",
        "      # Create attention masks\n",
        "      attention_masks = []\n",
        "      # Create a mask of 1s for each token followed by 0s for padding\n",
        "      for seq in input_ids:\n",
        "        seq_mask = [float(i > 0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "      # Convert all of our data into torch tensors, the required datatype for our model\n",
        "      inputs = torch.tensor(input_ids)\n",
        "      labels = torch.tensor(labels)\n",
        "      masks = torch.tensor(attention_masks)\n",
        "      return inputs, labels, masks\n",
        "  def data_prepare_BERT_test(self,X_train):\n",
        "      # Create sentence and label lists\n",
        "      sentences = X_train.values\n",
        "      sentences = ['[CLS] ' + sentence + ' [SEP]' for sentence in\n",
        "                 sentences]\n",
        "     \n",
        "      # Import the BERT tokenizer, used to convert our text into tokens that correspond to BERT's vocabulary.\n",
        "      tokenized_texts = [self.tokenizer.tokenize(sent) for sent in sentences]\n",
        "      # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "      input_ids = [self.tokenizer.convert_tokens_to_ids(x) for x in\n",
        "                 tokenized_texts]\n",
        "      # Pad our input seqeunce to the fixed length (i.e., max_len) with index of [PAD] token\n",
        "      # ~ input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "      pad_ind = self.tokenizer.convert_tokens_to_ids(['[PAD]'])[0]\n",
        "      input_ids = pad_sequences(\n",
        "          input_ids,\n",
        "          maxlen=self.max_seq_length + 2,\n",
        "          dtype='long',\n",
        "          truncating='post',\n",
        "          padding='post',\n",
        "          value=pad_ind,\n",
        "          )\n",
        "      # Create attention masks\n",
        "      attention_masks = []\n",
        "      # Create a mask of 1s for each token followed by 0s for padding\n",
        "      for seq in input_ids:\n",
        "        seq_mask = [float(i > 0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "      # Convert all of our data into torch tensors, the required datatype for our model\n",
        "      inputs = torch.tensor(input_ids)\n",
        "      masks = torch.tensor(attention_masks)\n",
        "      return inputs,  masks\n",
        "  def eval(self,X_eval,y_eval, data_set='DEV'):\n",
        "\n",
        "    \"\"\"Evaluate the trained model on a given data set.\n",
        "        Args:\n",
        "            X_eval (:obj:`np array or pandas series`, optional): loaded data for evaluation.\n",
        "\n",
        "            y_eval (:obj:`np array or pandas series`, optional): loaded labels for evaluation.\n",
        "\n",
        "            data_set (:obj:`str`, optional): Name of the provided data set to\n",
        "                use. This is ignored if data_path is not None. Can be either\n",
        "                'VALIDATION' or 'TEST'. Defaults to 'VALIDATION'.\n",
        "        Returns:\n",
        "            :obj:`dict`: A dictionary mapping an evaluation metric to its\n",
        "            computed value. The metrics used are accuracy, f1_micro, f1_macro,\n",
        "            recall_micro, recall_macro, precision_micro and precision_macro.\n",
        "        \"\"\"\n",
        "    validation_inputs, validation_labels, validation_masks = self.data_prepare_BERT(X_eval,y_eval)\n",
        "    validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "    validation_dataloader = DataLoader(validation_data, batch_size=self.batch_size)\n",
        "    self.model.eval()\n",
        "    all_pred=[]\n",
        "    all_label = []\n",
        "    with torch.no_grad():\n",
        "      for i, batch in enumerate(validation_dataloader):\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from our dataloader\n",
        "        input_ids, input_mask, labels = batch\n",
        "        outputs = self.model(input_ids, input_mask, labels=labels)\n",
        "        loss, logits = outputs[:2]\n",
        "        \n",
        "        probabilities, predicted = torch.max(logits.cpu().data, 1)\n",
        "        # put all the true labels and predictions to two lists\n",
        "        all_pred.extend(predicted)\n",
        "        all_label.extend(labels.cpu())\n",
        "    accuracy = accuracy_score(all_label, all_pred)\n",
        "    macro_f1_pos_neg = f1_score(all_label, all_pred,average='macro',labels=[0,1])\n",
        "    f1score = f1_score(all_label, all_pred, average='macro') \n",
        "    recall = recall_score(all_label, all_pred, average='macro')\n",
        "    precision = precision_score(all_label, all_pred, average='macro')\n",
        "    # Get scores\n",
        "    scores = {\n",
        "        'Sentiment': {\n",
        "            'accuracy': accuracy,\n",
        "            'f1_macro': f1score,\n",
        "            'recall_macro': recall,\n",
        "            'precision_macro':precision\n",
        "        }\n",
        "    }\n",
        "    return scores\n",
        "\n",
        "  def predict(self,sentences):\n",
        "    \"\"\"Predict the sentiment  probability scores for a given list of\n",
        "        sentences.\n",
        "        Args:\n",
        "            sentences (:obj:`list` of :obj:`str`): The list of sentences.\n",
        "            output (:obj:`str`): The output label type. Possible values are\n",
        "                'postive', 'neagtive', 'neutral'.\n",
        "        Returns:\n",
        "            :obj:`list` of :obj:`DIDPred`: A list of prediction results,\n",
        "            each corresponding to its respective sentence.\n",
        "        \"\"\"\n",
        "    if isinstance(sentences, str):\n",
        "      sentences=pd.Series(sentences)\n",
        "    validation_inputs, validation_masks = self.data_prepare_BERT_test(sentences)\n",
        "    validation_data = TensorDataset(validation_inputs, validation_masks)\n",
        "    validation_dataloader = DataLoader(validation_data, batch_size=self.batch_size)\n",
        "    self.model.eval()\n",
        "    all_pred=[]\n",
        "    result = collections.deque()\n",
        "    convert = lambda x: x\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      for i, batch in enumerate(validation_dataloader):\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from our dataloader\n",
        "        input_ids, input_mask = batch\n",
        "        outputs = self.model(input_ids.to(device), input_mask.to(device))\n",
        "        logits = outputs[:2][0]\n",
        "        \n",
        "        \n",
        "        \n",
        "        probabilities, predicted = torch.max(outputs[0].cpu().data, 1)\n",
        "        for i, val in enumerate(self.label2index):\n",
        "          if i==predicted.item():\n",
        "            all_pred.extend((val,probabilities.item()))\n",
        "            result.append(convert(DIDPred(val, logits.cpu().data.numpy().tolist())))\n",
        "            break\n",
        "        # put all the true labels and predictions to two lists\n",
        "\n",
        "        \n",
        "      \n",
        "    return list(result)\n",
        "        \n"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rnbIM_hYN1a"
      },
      "source": [
        "# Testing Production code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoqGSsUzYNUI"
      },
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/Omdena_sentiment/Dataset/final_text.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QxVgIOPYdJr"
      },
      "source": [
        "msk = np.random.rand(len(df)) < 0.7\n",
        "train = df[msk]\n",
        "test = df[~msk]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Donm5w_VYe0p"
      },
      "source": [
        "msk = np.random.rand(len(train)) < 0.8\n",
        "train_new = train[msk]\n",
        "valid = train[~msk]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJFtZwWsYg3d"
      },
      "source": [
        "labels_numeric=[0,1,2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNEqmzbUYivv",
        "outputId": "ed77b3ca-d812-419b-e48e-bcbe48432bca"
      },
      "source": [
        "train_new.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wfbdf1VYkVT",
        "outputId": "befe334f-cce1-4b99-c149-760badba39d8"
      },
      "source": [
        "test.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e7vZzyIYlha",
        "outputId": "3b6edd31-9099-4390-a96b-e982d4394481"
      },
      "source": [
        "valid.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdtYDdeLYnFR"
      },
      "source": [
        "MARBERT_Sentiment_Classifier=SentimentIdentificationMARBERT(labels_numeric,num_epoch=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOq6WKPhZJVn",
        "outputId": "f75b9e1e-5136-4aea-9b37-c1e65f8eeb57"
      },
      "source": [
        "MARBERT_Sentiment_Classifier.fine_tune(train_new['final'],train_new['label'],valid['final'],valid['label'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at /content/MARBERT_pytorch_verison were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/MARBERT_pytorch_verison and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Run with one GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlBNNEMRkySB"
      },
      "source": [
        "MARBERT_Sentiment_Classifier.save_model('/content/drive/MyDrive/Omdena_sentiment/Saved_models/Production/MARBERT')"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_HD_EB0v6j2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "outputId": "5f82cd38-fd97-4b81-c229-726ee84cf054"
      },
      "source": [
        "MARBERT_Sentiment_Classifier.save_label2ind_file('/content/drive/MyDrive/Omdena_sentiment/Saved_models/Production/MARBERT/_labels-dict.json')"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-a669b831a41f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mMARBERT_Sentiment_Classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_label2ind_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Omdena_sentiment/Saved_models/Production/MARBERT/_labels-dict.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-41-4ec5031ad25c>\u001b[0m in \u001b[0;36msave_label2ind_file\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     53\u001b[0m       \"\"\"\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabels_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mdata_prepare_BERT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'SentimentIdentificationMARBERT' object has no attribute 'abels_json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLHeA_gWRexV",
        "outputId": "3b55338e-9563-4693-bca1-f1e4ce772933"
      },
      "source": [
        "MARBERT_Sentiment_Classifier.eval(test['final'],test['label'])"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Sentiment': {'accuracy': 0.25316363530456815,\n",
              "  'f1_macro': 0.13469600809306,\n",
              "  'precision_macro': 0.4177175865249259,\n",
              "  'recall_macro': 0.3333437744714174}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXJGuqCBbj9b",
        "outputId": "ffb3bbab-b034-446a-e994-5abf3300bff9"
      },
      "source": [
        "Marbert_predictor=SentimentIdentificationMARBERTPrediction(labels_numeric)"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run with one GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2co8dizqbNjz",
        "outputId": "8148307e-e490-4ccb-93e7-a1fa9bec6c78"
      },
      "source": [
        "Marbert_predictor.predict('صفاء الهاشم سيده كويتيه المراه الوحيده حاليا مجلس الامه الكويتي مدافعه شرسه حقوق المراه وحق المواطن الكويتي وتمتلك عقليه اقتصاديه مميزه اختيرت ضمن سيده عربيه مؤsثره مجتمعها')"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[DIDPred(top='0', scores=[[4.2006402015686035, -1.7441024780273438, -2.107455015182495]])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    }
  ]
}