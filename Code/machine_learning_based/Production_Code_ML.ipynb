{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Production_Code_ML.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqI4mhQUguc2"
      },
      "source": [
        "import collections\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
        "import joblib\n",
        "import re\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "import pickle\n",
        "import dill\n"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pjhj_bHmdnw"
      },
      "source": [
        "_DEFAULT_LABELS=['negative','positive','neutral']"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS0IJAX1lnTH"
      },
      "source": [
        "class SentimentIdentificationML(object):\n",
        "\n",
        "  \"\"\"A class for training, evaluating and running the sentiment classification\n",
        "     After initializing an instance, you must\n",
        "    run the train method once before using it.\n",
        "    Args:\n",
        "        labels (:obj:`set` of :obj:`str`, optional): The set of sentiment labels\n",
        "            used in the training data in the main model.\n",
        "            If None, the default labels are used.\n",
        "            Defaults to None.\n",
        "       \n",
        "    \"\"\"\n",
        "  def __init__(self, labels=None,\n",
        "                 ):\n",
        "        if labels is None:\n",
        "            labels = _DEFAULT_LABELS\n",
        "        self._labels_sorted = sorted(labels)\n",
        "        self._is_trained = False\n",
        "\n",
        "  def _apply_preprocessing(self,text):\n",
        "\n",
        "      # ref: https://github.com/bakrianoo/aravec\n",
        "      search = [\"أ\",\"إ\",\"آ\",\"ة\",\"_\",\"-\",\"/\",\".\",\"،\",\" و \",\" يا \",'\"',\"ـ\",\"'\",\"ى\",\n",
        "                \"\\\\\",'\\n', '\\t','&quot;','?','؟','!']\n",
        "      replace = [\"ا\",\"ا\",\"ا\",\"ه\",\" \",\" \",\"\",\"\",\"\",\" و\",\" يا\",\n",
        "                \"\",\"\",\"\",\"ي\",\"\",' ', ' ',' ',' ? ',' ؟ ', ' ! ']\n",
        "      \n",
        "      tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
        "      text = re.sub(tashkeel,\"\", text)\n",
        "      \n",
        "      longation = re.compile(r'(.)\\1+')\n",
        "      subst = r\"\\1\\1\"\n",
        "      text = re.sub(longation, subst, text)\n",
        "      \n",
        "      text = re.sub(r\"[^\\w\\s]\", '', text)\n",
        "      text = re.sub(r\"[a-zA-Z]\", '', text)\n",
        "      text = re.sub(r\"\\d+\", ' ', text)\n",
        "      text = re.sub(r\"\\n+\", ' ', text)\n",
        "      text = re.sub(r\"\\t+\", ' ', text)\n",
        "      text = re.sub(r\"\\r+\", ' ', text)\n",
        "      text = re.sub(r\"\\s+\", ' ', text)\n",
        "      text = text.replace('وو', 'و')\n",
        "      text = text.replace('يي', 'ي')\n",
        "      text = text.replace('اا', 'ا')\n",
        "      \n",
        "      for i in range(0, len(search)):\n",
        "          text = text.replace(search[i], replace[i])\n",
        "      \n",
        "      text = text.strip()\n",
        "\n",
        "\n",
        "      \n",
        "      return text\n",
        "\n",
        "\n",
        "  def _prepare_sentences(self, sentences):\n",
        "      cleaned_sent=  sentences.apply(self._apply_preprocessing)             \n",
        "      sent_array = np.array(cleaned_sent)\n",
        "      if not self._is_trained:\n",
        "        x_trans = self._feat_union.fit_transform(sent_array)\n",
        "      \n",
        "      else:\n",
        "        x_trans = self._feat_union.transform(sent_array)\n",
        "\n",
        "      return x_trans\n",
        "\n",
        "  def  use_defult_fearure_vec(self):\n",
        "\n",
        "      \n",
        "      union = FeatureUnion([(\"tf_idf_ngra_3\", TfidfVectorizer(min_df=1,ngram_range=(1, 3),binary=True,max_features=700)),\n",
        "                        ( \"tf_idf_ngra_2\", TfidfVectorizer(min_df=1,ngram_range=(1, 2),binary=True,max_features=700)),\n",
        "                      (\"tf_idf_ngra_1\",TfidfVectorizer(min_df=1,ngram_range=(1, 1),binary=True,max_features=700)),\n",
        "                      (\"tf_idf_ngra_5\", TfidfVectorizer(min_df=1,ngram_range=(1, 5),binary=True,max_features=700)),\n",
        "                      (\"tf_idf_ngra_7\", TfidfVectorizer(min_df=1,ngram_range=(1, 7),binary=True,max_features=700)),\n",
        "                      (\"tf_idf_ngra_3_ch\",TfidfVectorizer(min_df=1,ngram_range=(1, 3),binary=True,max_features=700,analyzer='char')),\n",
        "                      (\"tf_idf_ngra_2_ch\",TfidfVectorizer(min_df=1,ngram_range=(1, 2),binary=True,max_features=700,analyzer='char')),\n",
        "                      (\"tf_idf_ngra_1_ch\",TfidfVectorizer(min_df=1,ngram_range=(1, 1),binary=True,max_features=700,analyzer='char')),\n",
        "                      (\"tf_idf_ngra_5_ch\",TfidfVectorizer(min_df=1,ngram_range=(1, 5),binary=True,max_features=700,analyzer='char')),\n",
        "                      (\"tf_idf_ngra_7_ch\", TfidfVectorizer(min_df=1,ngram_range=(1, 7),binary=True,max_features=700,analyzer='char')),\n",
        "                      (\"cnt_vec_ngra_3\", CountVectorizer(min_df=1,ngram_range=(1, 3),binary=True,max_features=700)),\n",
        "                      (\"cnt_vec_ngra_5\", CountVectorizer(min_df=1,ngram_range=(1, 5),binary=True,max_features=700))\n",
        "                                      \n",
        "                                      \n",
        "                                      ])\n",
        "      self._feat_union = union\n",
        "\n",
        "  def use_recomended_feature_vec(self,char_ngram_range,word_ngram_range):\n",
        "      word_vectorizer = TfidfVectorizer(lowercase=False,\n",
        "                                          ngram_range=word_ngram_range,\n",
        "                                          analyzer='word',\n",
        "                                          tokenizer=lambda x: x.split(' '))\n",
        "      char_vectorizer = TfidfVectorizer(lowercase=False,\n",
        "                                          ngram_range=char_ngram_range,\n",
        "                                          analyzer='char',\n",
        "                                          tokenizer=lambda x: x.split(' '))\n",
        "      self._feat_union = FeatureUnion([('wordgrams', word_vectorizer),\n",
        "                                               ('chargrams', char_vectorizer)])\n",
        "        \n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "  def train(self, X_train,\n",
        "              y_train,\n",
        "              use_defult_fearures=False,\n",
        "              char_ngram_range=(1, 3),\n",
        "              word_ngram_range=(1, 1),\n",
        "              n_jobs=None):\n",
        "        \"\"\"Trains the model on a given data set.\n",
        "        Args:\n",
        "            X_train (:obj:`np array or pandas series`, optional): loaded training data.\n",
        "               \n",
        "            y_train (:obj:`np array or pandas series`, optional): loaded labels for training.\n",
        "\n",
        "            use_defult_fearures (:obj:'bool', optional): Use recommended feature vector \n",
        "\n",
        "\n",
        "            char_ngram_range (:obj:`tuple`, optional): The n-gram ranges to\n",
        "                consider in the character-based language models.\n",
        "                Defaults to (1, 3).\n",
        "            word_ngram_range (:obj:`tuple`, optional): The n-gram ranges to\n",
        "                consider in the word-based language models.\n",
        "                Defaults to (1, 1).\n",
        "            n_jobs (:obj:`int`, optional): The number of parallel jobs to use\n",
        "                for computation. If None, then only 1 job is used.\n",
        "                If -1 then all processors are used. Defaults to None.\n",
        "        \"\"\"\n",
        "\n",
        "       \n",
        "\n",
        "        \n",
        "       \n",
        "\n",
        "        # Build and train extra classifier\n",
        "        if use_defult_fearures:\n",
        "          self.use_defult_fearure_vec()\n",
        "        else:\n",
        "          self.use_recomended_feature_vec(char_ngram_range,word_ngram_range)   \n",
        "\n",
        "        x_prepared = self._prepare_sentences(X_train)    \n",
        "       \n",
        "       \n",
        "\n",
        "        self._classifier = CalibratedClassifierCV(LinearSVC(C=0.01, class_weight='balanced', penalty='l2'))\n",
        "        self._classifier.fit(x_prepared, y_train)\n",
        "        self._is_trained = True\n",
        "\n",
        "  def eval(self,  X_eval,\n",
        "              y_eval, data_set='DEV'):\n",
        "        \"\"\"Evaluate the trained model on a given data set.\n",
        "        Args:\n",
        "            X_eval (:obj:`np array or pandas series`, optional): loaded data for evaluation.\n",
        "\n",
        "            y_eval (:obj:`np array or pandas series`, optional): loaded labels for evaluation.\n",
        "\n",
        "            data_set (:obj:`str`, optional): Name of the provided data set to\n",
        "                use. This is ignored if data_path is not None. Can be either\n",
        "                'VALIDATION' or 'TEST'. Defaults to 'VALIDATION'.\n",
        "        Returns:\n",
        "            :obj:`dict`: A dictionary mapping an evaluation metric to its\n",
        "            computed value. The metrics used are accuracy, f1_micro, f1_macro,\n",
        "            recall_micro, recall_macro, precision_micro and precision_macro.\n",
        "        \"\"\"\n",
        "\n",
        "        if not self._is_trained:\n",
        "            raise UntrainedModelError(\n",
        "                'Can\\'t evaluate an untrained model.')\n",
        "\n",
        "       \n",
        "\n",
        "        # # prepare data\n",
        "        # eval_data = self._prepare_sentences(X_eval)\n",
        "        \n",
        "\n",
        "        # Generate predictions\n",
        "        did_pred = self.predict(X_eval)\n",
        "        did_pred = [d.top for d in did_pred]\n",
        "       \n",
        "\n",
        "        # Get scores\n",
        "        scores = {\n",
        "            'Sentiment': {\n",
        "                'accuracy': accuracy_score(y_eval, did_pred),\n",
        "                'f1_macro': f1_score(y_eval, did_pred,\n",
        "                                     average='macro'),\n",
        "                'recall_macro': recall_score(y_eval, did_pred,\n",
        "                                             average='macro'),\n",
        "                'precision_macro': precision_score(y_eval,\n",
        "                                                   did_pred,\n",
        "                                                   average='macro')\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return scores    \n",
        "\n",
        "  def predict(self, sentences, output='label'):\n",
        "        \"\"\"Predict the sentiment  probability scores for a given list of\n",
        "        sentences.\n",
        "        Args:\n",
        "            sentences (:obj:`list` of :obj:`str`): The list of sentences.\n",
        "            output (:obj:`str`): The output label type. Possible values are\n",
        "                'postive', 'neagtive', 'neutral'.\n",
        "        Returns:\n",
        "            :obj:`list` of :obj:`DIDPred`: A list of prediction results,\n",
        "            each corresponding to its respective sentence.\n",
        "        \"\"\"\n",
        "\n",
        "        if not self._is_trained:\n",
        "            raise UntrainedModelError(\n",
        "                'Can\\'t predict with an untrained model.')\n",
        "\n",
        "        \n",
        "        if isinstance(sentences, str):\n",
        "          sentences=pd.Series(sentences)\n",
        "        x_prepared = self._prepare_sentences(sentences)\n",
        "        predicted_scores = self._classifier.predict_proba(x_prepared)\n",
        "        convert = lambda x: x\n",
        "\n",
        "        result = collections.deque()\n",
        "        for scores in predicted_scores:\n",
        "            score_tups = list(zip(self._labels_sorted, scores))\n",
        "            predicted_sentiment = max(score_tups, key=lambda x: x[1])[0]\n",
        "            sentiment_scores = dict(score_tups)\n",
        "            result.append(convert(DIDPred(predicted_sentiment, sentiment_scores)))\n",
        "\n",
        "        return list(result)\n",
        "\n",
        "  def save_model(self,path):\n",
        "      \"\"\"Save  the model on a given data set.\n",
        "        Args:\n",
        "            Path (:obj:`str`): Path where you want to save the model.\n",
        "               \n",
        "           \n",
        "        \"\"\"\n",
        "      joblib.dump(self._classifier, path+'/svm_model.sav')\n",
        "  def save_feature_vec(self,path):\n",
        "    \"\"\"Save  the feature vector on a given data set.\n",
        "      Args:\n",
        "          Path (:obj:`str`): Path where you want to save the feature vector .\n",
        "              \n",
        "          \n",
        "      \"\"\"\n",
        "    \n",
        "    # torch.save(obj=self._feat_union,\n",
        "    #     f=path+'/feat_vec.pkl',\n",
        "    #     pickle_module=dill\n",
        "    # )\n",
        "    dill.dump(self._feat_union, open( path+\"/feat_vec.p\", \"wb\" ))\n",
        "    # pickle.dump(self._feat_union.get_params(), open( path+\"/feat_vec.p\", \"wb\" ) )\n",
        "    # joblib.dump(pipeline, 'filename.pkl', compress = 1)\n",
        "\n",
        "    # joblib.dump(self._feat_union.get_params(), path+'/feat_vec.pkl', compress = 1)\n",
        "\n",
        "\n"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uB90Yw5-qu9x"
      },
      "source": [
        "class UntrainedModelError(SentimentIdentificationML):\n",
        "    \"\"\"Error thrown when attempting to use an untrained sentiment classifier\n",
        "    instance.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, msg):\n",
        "        DialectIdError.__init__(self, msg)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaNq_xio_mxC"
      },
      "source": [
        "class DIDPred(collections.namedtuple('DIDPred', ['top', 'scores'])):\n",
        "    \"\"\"A named tuple containing sentiment ID prediction results.\n",
        "    Attributes:\n",
        "        top (:obj:`str`): The sentiment label with the highest score. See\n",
        "            :ref:`sentimentid_labels` for a list of output labels.\n",
        "        scores (:obj:`dict`): A dictionary mapping each sentiment label to it's\n",
        "            computed score.\n",
        "    \"\"\""
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COTWDOdBuLsA"
      },
      "source": [
        "MODEL_PATH='/content/drive/MyDrive/Omdena_sentiment/Saved_models/Production/svm_model.sav'\n",
        "FEATURE_VEC_PATH='/content/drive/MyDrive/Omdena_sentiment/Saved_models/Production/feat_vec.p'"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_fuj2VgsnI7"
      },
      "source": [
        "class SentimentIdentificationMLInfereing(object):\n",
        "  \"\"\"A class for running the sentiment classification based on pretrained ML model\n",
        "     \n",
        "    Args:\n",
        "        labels (:obj:`set` of :obj:`str`, optional): The set of dialect labels\n",
        "            used in the training data in the main model.\n",
        "            If None, the default labels are used.\n",
        "            Defaults to None.\n",
        "\n",
        "        training_model_path (:obj:`str`, optional): Path of training model to be used for inference,\n",
        "        If none, use defult model for this libaray\n",
        "        feature_vector_path (:obj:`str`, optional): Path of feature vector to be used for preparing data,\n",
        "        If none, use defult model for this libaray\n",
        "       \n",
        "    \"\"\"\n",
        "  def __init__(self, labels=None,training_model_path=None,feature_vector_path=None,\n",
        "                 ):\n",
        "        if labels is None:\n",
        "            labels = _DEFAULT_LABELS\n",
        "        self._labels_sorted = sorted(labels)\n",
        "        if feature_vector_path is None:\n",
        "          self._feat_union=dill.load(open(FEATURE_VEC_PATH, 'rb'))\n",
        "          # self._feat_union_params=  joblib.load(FEATURE_VEC_PATH)\n",
        "          # self._feat_union=FeatureUnion()\n",
        "          # self._feat_union.set_params(self._feat_union_params)\n",
        "        else:\n",
        "          self._feat_union=  dill.load(open(feature_vector_path, 'rb'))\n",
        "        if training_model_path is None:\n",
        "          self._classifier=  joblib.load(MODEL_PATH)\n",
        "        else:\n",
        "          self._classifier=  joblib.load(training_model_path)\n",
        "\n",
        "  def _apply_preprocessing(self,text):\n",
        "\n",
        "      # ref: https://github.com/bakrianoo/aravec\n",
        "      search = [\"أ\",\"إ\",\"آ\",\"ة\",\"_\",\"-\",\"/\",\".\",\"،\",\" و \",\" يا \",'\"',\"ـ\",\"'\",\"ى\",\n",
        "                \"\\\\\",'\\n', '\\t','&quot;','?','؟','!']\n",
        "      replace = [\"ا\",\"ا\",\"ا\",\"ه\",\" \",\" \",\"\",\"\",\"\",\" و\",\" يا\",\n",
        "                \"\",\"\",\"\",\"ي\",\"\",' ', ' ',' ',' ? ',' ؟ ', ' ! ']\n",
        "      \n",
        "      tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
        "      text = re.sub(tashkeel,\"\", text)\n",
        "      \n",
        "      longation = re.compile(r'(.)\\1+')\n",
        "      subst = r\"\\1\\1\"\n",
        "      text = re.sub(longation, subst, text)\n",
        "      \n",
        "      text = re.sub(r\"[^\\w\\s]\", '', text)\n",
        "      text = re.sub(r\"[a-zA-Z]\", '', text)\n",
        "      text = re.sub(r\"\\d+\", ' ', text)\n",
        "      text = re.sub(r\"\\n+\", ' ', text)\n",
        "      text = re.sub(r\"\\t+\", ' ', text)\n",
        "      text = re.sub(r\"\\r+\", ' ', text)\n",
        "      text = re.sub(r\"\\s+\", ' ', text)\n",
        "      text = text.replace('وو', 'و')\n",
        "      text = text.replace('يي', 'ي')\n",
        "      text = text.replace('اا', 'ا')\n",
        "      \n",
        "      for i in range(0, len(search)):\n",
        "          text = text.replace(search[i], replace[i])\n",
        "      \n",
        "      text = text.strip()\n",
        "\n",
        "\n",
        "      \n",
        "      return text\n",
        "\n",
        "\n",
        "  def _prepare_sentences(self, sentences):\n",
        "\n",
        "        \n",
        "      cleaned_sent=  sentences.apply(self._apply_preprocessing)             \n",
        "      sent_array = np.array(cleaned_sent)       \n",
        "      x_trans = self._feat_union.transform(sent_array)        \n",
        "      return x_trans\n",
        "\n",
        "\n",
        "  def eval(self,  X_eval,\n",
        "              y_eval, data_set='DEV'):\n",
        "        \"\"\"Evaluate the trained model on a given data set.\n",
        "        Args:\n",
        "            X_eval (:obj:`np array or pandas series`, optional): loaded data for evaluation.\n",
        "\n",
        "            y_eval (:obj:`np array or pandas series`, optional): loaded labels for evaluation.\n",
        "\n",
        "            data_set (:obj:`str`, optional): Name of the provided data set to\n",
        "                use. This is ignored if data_path is not None. Can be either\n",
        "                'VALIDATION' or 'TEST'. Defaults to 'VALIDATION'.\n",
        "        Returns:\n",
        "            :obj:`dict`: A dictionary mapping an evaluation metric to its\n",
        "            computed value. The metrics used are accuracy, f1_micro, f1_macro,\n",
        "            recall_micro, recall_macro, precision_micro and precision_macro.\n",
        "        \"\"\"\n",
        "\n",
        "        \n",
        "\n",
        "        # # prepare data\n",
        "        # eval_data = self._prepare_sentences(X_eval)\n",
        "        \n",
        "\n",
        "        # Generate predictions\n",
        "        did_pred = self.predict(X_eval)\n",
        "        did_pred = [d.top for d in did_pred]\n",
        "        \n",
        "       \n",
        "\n",
        "        # Get scores\n",
        "        scores = {\n",
        "            'Sentiment': {\n",
        "                'accuracy': accuracy_score(y_eval, did_pred),\n",
        "                'f1_macro': f1_score(y_eval, did_pred,\n",
        "                                     average='macro'),\n",
        "                'recall_macro': recall_score(y_eval, did_pred,\n",
        "                                             average='macro'),\n",
        "                'precision_macro': precision_score(y_eval,\n",
        "                                                   did_pred,\n",
        "                                                   average='macro')\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return scores    \n",
        "\n",
        "  def predict(self, sentences, output='label'):\n",
        "        \"\"\"Predict the sentiment  probability scores for a given list of\n",
        "        sentences.\n",
        "        Args:\n",
        "            sentences (:obj:`list` of :obj:`str`): The list of sentences.\n",
        "            output (:obj:`str`): The output label type. Possible values are\n",
        "                'postive', 'neagtive', 'neutral'.\n",
        "        Returns:\n",
        "            :obj:`list` of :obj:`DIDPred`: A list of prediction results,\n",
        "            each corresponding to its respective sentence.\n",
        "        \"\"\"\n",
        "        if isinstance(sentences, str):\n",
        "          sentences=pd.Series(sentences)\n",
        "        x_prepared = self._prepare_sentences(sentences)\n",
        "        predicted_scores = self._classifier.predict_proba(x_prepared)\n",
        "        convert = lambda x: x\n",
        "\n",
        "        result = collections.deque()\n",
        "        for scores in predicted_scores:\n",
        "            score_tups = list(zip(self._labels_sorted, scores))\n",
        "            predicted_sentiment = max(score_tups, key=lambda x: x[1])[0]\n",
        "            sentiment_scores = dict(score_tups)\n",
        "            result.append(convert(DIDPred(predicted_sentiment, sentiment_scores)))\n",
        "\n",
        "        return list(result)"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbwFPq0WvtgO"
      },
      "source": [
        "# Testing Production Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au3mq-8ZvxUL"
      },
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/Omdena_sentiment/Dataset/final_text.csv')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "zFrnaGMnyjqb",
        "outputId": "bbf723c8-f8a6-426e-868e-919fd24620df"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>final</th>\n",
              "      <th>label</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>رجل يرفع شعار الحريه يدعو لرفع الظلم المراه او...</td>\n",
              "      <td>2</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>صفاء الهاشم سيده كويتيه المراه الوحيده حاليا م...</td>\n",
              "      <td>2</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>حقوق المراه عينك مو دكتور واحدحثاله بلدنا بلد ...</td>\n",
              "      <td>2</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>شخصيا حقوق المراه لانو بالجد الدستور السوداني ...</td>\n",
              "      <td>2</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>حق حقوق المراه مثل الرجل يريد وحده زينه واخلاق...</td>\n",
              "      <td>2</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               final  label  length\n",
              "0  رجل يرفع شعار الحريه يدعو لرفع الظلم المراه او...      2      31\n",
              "1  صفاء الهاشم سيده كويتيه المراه الوحيده حاليا م...      2      27\n",
              "2  حقوق المراه عينك مو دكتور واحدحثاله بلدنا بلد ...      2      40\n",
              "3  شخصيا حقوق المراه لانو بالجد الدستور السوداني ...      2      42\n",
              "4  حق حقوق المراه مثل الرجل يريد وحده زينه واخلاق...      2      21"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-782TuMymhs"
      },
      "source": [
        "msk = np.random.rand(len(df)) < 0.7\n",
        "train = df[msk]\n",
        "test = df[~msk]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXo0JllnyxZC"
      },
      "source": [
        "msk = np.random.rand(len(train)) < 0.8\n",
        "train_new = train[msk]\n",
        "valid = train[~msk]\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "8_bZkdaAy2zl",
        "outputId": "b51255fe-4b29-489e-e9f8-bf40d0e92490"
      },
      "source": [
        "train_new.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>final</th>\n",
              "      <th>label</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>رجل يرفع شعار الحريه يدعو لرفع الظلم المراه او...</td>\n",
              "      <td>2</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>حقوق المراه عينك مو دكتور واحدحثاله بلدنا بلد ...</td>\n",
              "      <td>2</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>خيركم خيركم لاهله تكثر مخاطبه الرجال حقوق المر...</td>\n",
              "      <td>2</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>اليوم دخلت نقاش حلو حقوق المراه و دكتور مطوع ض...</td>\n",
              "      <td>2</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>صباح النيات المول انعاشها وتحقيقها يدي سمو سيد...</td>\n",
              "      <td>2</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               final  label  length\n",
              "0  رجل يرفع شعار الحريه يدعو لرفع الظلم المراه او...      2      31\n",
              "2  حقوق المراه عينك مو دكتور واحدحثاله بلدنا بلد ...      2      40\n",
              "7  خيركم خيركم لاهله تكثر مخاطبه الرجال حقوق المر...      2      27\n",
              "8  اليوم دخلت نقاش حلو حقوق المراه و دكتور مطوع ض...      2      22\n",
              "9  صباح النيات المول انعاشها وتحقيقها يدي سمو سيد...      2      36"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gru2nOBb4PKp",
        "outputId": "a0ae3220-59dc-42e5-d53c-4c7267fcb9ee"
      },
      "source": [
        "len(train_new)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128770"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Nu67XY7fy5uo",
        "outputId": "e6d6f675-f0c2-4f5e-c893-34441af0c5dd"
      },
      "source": [
        "valid.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>final</th>\n",
              "      <th>label</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>تطور كبير عم حسه شان حقوق واجبات المراه الامار...</td>\n",
              "      <td>2</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>التعدد مطلبنا اشوف تعدد الزوجات يتسبب سعاده ال...</td>\n",
              "      <td>2</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>دراسات علميه اوضحت دراسات علميه بان تعدد الزوج...</td>\n",
              "      <td>2</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>اجمل سعوديه ولامغربيه السعوديه المغربيه تكفي ل...</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>تعدد الزوجات فوائد والمستفيد الاول زوجته الاول...</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                final  label  length\n",
              "10  تطور كبير عم حسه شان حقوق واجبات المراه الامار...      2      17\n",
              "12  التعدد مطلبنا اشوف تعدد الزوجات يتسبب سعاده ال...      2      21\n",
              "15  دراسات علميه اوضحت دراسات علميه بان تعدد الزوج...      2      39\n",
              "18  اجمل سعوديه ولامغربيه السعوديه المغربيه تكفي ل...      2      16\n",
              "27  تعدد الزوجات فوائد والمستفيد الاول زوجته الاول...      2       9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "rsNmesKxy9mZ",
        "outputId": "16f86619-c865-4494-ad88-3cafe607f619"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>final</th>\n",
              "      <th>label</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>صفاء الهاشم سيده كويتيه المراه الوحيده حاليا م...</td>\n",
              "      <td>2</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>حقوق المراه عينك مو دكتور واحدحثاله بلدنا بلد ...</td>\n",
              "      <td>2</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>قرارات عدليه هامه عززت حقوق المراه</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>التعدد مطلبنا تعدد الزوجات حق مشروع حقوق الزوج...</td>\n",
              "      <td>2</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>تعدد الزوجات نعمه مغبون كثير الناس عددوا واعدل...</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                final  label  length\n",
              "1   صفاء الهاشم سيده كويتيه المراه الوحيده حاليا م...      2      27\n",
              "2   حقوق المراه عينك مو دكتور واحدحثاله بلدنا بلد ...      2      40\n",
              "5                  قرارات عدليه هامه عززت حقوق المراه      2       6\n",
              "13  التعدد مطلبنا تعدد الزوجات حق مشروع حقوق الزوج...      2      23\n",
              "14  تعدد الزوجات نعمه مغبون كثير الناس عددوا واعدل...      2      11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ALfey7PzRuf"
      },
      "source": [
        "labels_numeric=[0,1,2]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-oYgitK1Ef3",
        "outputId": "0f8c8529-79ef-43e0-eb14-a33047a6a9d0"
      },
      "source": [
        "train_new.dropna(inplace=True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idmKwjwpVJqW",
        "outputId": "75bd4aad-32df-4f93-cd50-5a81df4884d6"
      },
      "source": [
        "test.dropna(inplace=True)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXHwC95jzCKq"
      },
      "source": [
        "sentimenclassifier=SentimentIdentificationML(labels_numeric)"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5Bi5gUVziJU"
      },
      "source": [
        "sentimenclassifier.train(train_new['final'],train_new['label'],use_defult_fearures=False)"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwrbK7KC4qc4",
        "outputId": "007e3057-96ef-43e4-e26d-6c56bb440d3b"
      },
      "source": [
        "valid.dropna(inplace=True)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0gHznhnI48Y3",
        "outputId": "de7e2c54-5a90-4125-f2b9-2d6ab5e25208"
      },
      "source": [
        "valid.head()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>final</th>\n",
              "      <th>label</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>تطور كبير عم حسه شان حقوق واجبات المراه الامار...</td>\n",
              "      <td>2</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>رايكم تعدد الزوجات والله شي حلو يوم تذوق شي</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>تعدد الزوجات باب عظيم ابواب السعاده والخير وال...</td>\n",
              "      <td>2</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>خلاص الحمداله فرج الله عهد ملكنا سلمان عهده تو...</td>\n",
              "      <td>2</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>بارك الله امثالها وشافاها الله والدها الفتاه ا...</td>\n",
              "      <td>2</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                final  label  length\n",
              "10  تطور كبير عم حسه شان حقوق واجبات المراه الامار...      2      17\n",
              "16        رايكم تعدد الزوجات والله شي حلو يوم تذوق شي      2       9\n",
              "25  تعدد الزوجات باب عظيم ابواب السعاده والخير وال...      2      19\n",
              "40  خلاص الحمداله فرج الله عهد ملكنا سلمان عهده تو...      2      23\n",
              "53  بارك الله امثالها وشافاها الله والدها الفتاه ا...      2      17"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qwb1AcA1PJZ",
        "outputId": "d8cef787-bdb4-4fd5-c540-7d6ca8f6f6e5"
      },
      "source": [
        "sentimenclassifier.eval(valid['final'],valid['label'])"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Sentiment': {'accuracy': 0.7109357946926939,\n",
              "  'f1_macro': 0.6726495586133651,\n",
              "  'precision_macro': 0.7134542054825949,\n",
              "  'recall_macro': 0.6676578272000081}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "HNMsEJgeD3Vl",
        "outputId": "c6cb93bb-1ab1-41e1-8df7-25561f2907a5"
      },
      "source": [
        "test.iloc[0]['final']"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'صفاء الهاشم سيده كويتيه المراه الوحيده حاليا مجلس الامه الكويتي مدافعه شرسه حقوق المراه وحق المواطن الكويتي وتمتلك عقليه اقتصاديه مميزه اختيرت ضمن سيده عربيه مؤثره مجتمعها'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAb3IS6nDux_",
        "outputId": "3f96ffde-5671-4f95-b708-2410f5137618"
      },
      "source": [
        "sentimenclassifier.predict('صفاء الهاشم سيده كويتيه المراه الوحيده حاليا مجلس الامه الكويتي مدافعه شرسه حقوق المراه وحق المواطن الكويتي وتمتلك عقليه اقتصاديه مميزه اختيرت ضمن سيده عربيه مؤثره مجتمعها')"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[DIDPred(top=2, scores={0: 0.3142449591766248, 1: 0.2719087116949098, 2: 0.4138463291284653})]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZfDwe1VGSOZ"
      },
      "source": [
        "sentimenclassifier.save_feature_vec('/content/drive/MyDrive/Omdena_sentiment/Saved_models/Production')"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hW-Cjf62GThL"
      },
      "source": [
        "sentimenclassifier.save_model('/content/drive/MyDrive/Omdena_sentiment/Saved_models/Production')"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYpcX03vUmT4"
      },
      "source": [
        "InfereSentiment=SentimentIdentificationMLInfereing(labels_numeric)"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fu-Z-WA7VFmd",
        "outputId": "ab570c17-1628-448b-8ae5-3dbd6f7072e4"
      },
      "source": [
        "InfereSentiment.eval(test['final'],test['label'])"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Sentiment': {'accuracy': 0.7154780847678287,\n",
              "  'f1_macro': 0.6785456751911916,\n",
              "  'precision_macro': 0.7187791701771257,\n",
              "  'recall_macro': 0.6727025645388777}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    }
  ]
}