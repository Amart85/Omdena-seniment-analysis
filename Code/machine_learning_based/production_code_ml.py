# -*- coding: utf-8 -*-
"""Production_Code_ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11LTvJFVFBpF-LeM9e8ucuYlh9iHQiIuM
"""

import collections
from pathlib import Path
import sys
import numpy as np
import pandas as pd
import scipy as sp
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer
from sklearn.pipeline import FeatureUnion
from sklearn.multiclass import OneVsRestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.preprocessing import normalize
from sklearn.metrics import accuracy_score, f1_score, recall_score
from sklearn.metrics import precision_score
from sklearn.svm import SVC, LinearSVC
from sklearn.linear_model import SGDClassifier, LogisticRegression
import joblib
import re
from nltk.classify.scikitlearn import SklearnClassifier
from sklearn.calibration import CalibratedClassifierCV
import pickle
import dill

_DEFAULT_LABELS=['negative','positive','neutral']

class SentimentIdentificationML(object):

  """A class for training, evaluating and running the sentiment classification
     After initializing an instance, you must
    run the train method once before using it.
    Args:
        labels (:obj:`set` of :obj:`str`, optional): The set of sentiment labels
            used in the training data in the main model.
            If None, the default labels are used.
            Defaults to None.
       
    """
  def __init__(self, labels=None,
                 ):
        if labels is None:
            labels = _DEFAULT_LABELS
        self._labels_sorted = sorted(labels)
        self._is_trained = False

  def _apply_preprocessing(self,text):

      # ref: https://github.com/bakrianoo/aravec
      search = ["أ","إ","آ","ة","_","-","/",".","،"," و "," يا ",'"',"ـ","'","ى",
                "\\",'\n', '\t','&quot;','?','؟','!']
      replace = ["ا","ا","ا","ه"," "," ","","",""," و"," يا",
                "","","","ي","",' ', ' ',' ',' ? ',' ؟ ', ' ! ']
      
      tashkeel = re.compile(r'[\u0617-\u061A\u064B-\u0652]')
      text = re.sub(tashkeel,"", text)
      
      longation = re.compile(r'(.)\1+')
      subst = r"\1\1"
      text = re.sub(longation, subst, text)
      
      text = re.sub(r"[^\w\s]", '', text)
      text = re.sub(r"[a-zA-Z]", '', text)
      text = re.sub(r"\d+", ' ', text)
      text = re.sub(r"\n+", ' ', text)
      text = re.sub(r"\t+", ' ', text)
      text = re.sub(r"\r+", ' ', text)
      text = re.sub(r"\s+", ' ', text)
      text = text.replace('وو', 'و')
      text = text.replace('يي', 'ي')
      text = text.replace('اا', 'ا')
      
      for i in range(0, len(search)):
          text = text.replace(search[i], replace[i])
      
      text = text.strip()


      
      return text


  def _prepare_sentences(self, sentences):
      cleaned_sent=  sentences.apply(self._apply_preprocessing)             
      sent_array = np.array(cleaned_sent)
      if not self._is_trained:
        x_trans = self._feat_union.fit_transform(sent_array)
      
      else:
        x_trans = self._feat_union.transform(sent_array)

      return x_trans

  def  use_defult_fearure_vec(self):

      
      union = FeatureUnion([("tf_idf_ngra_3", TfidfVectorizer(min_df=1,ngram_range=(1, 3),binary=True,max_features=700)),
                        ( "tf_idf_ngra_2", TfidfVectorizer(min_df=1,ngram_range=(1, 2),binary=True,max_features=700)),
                      ("tf_idf_ngra_1",TfidfVectorizer(min_df=1,ngram_range=(1, 1),binary=True,max_features=700)),
                      ("tf_idf_ngra_5", TfidfVectorizer(min_df=1,ngram_range=(1, 5),binary=True,max_features=700)),
                      ("tf_idf_ngra_7", TfidfVectorizer(min_df=1,ngram_range=(1, 7),binary=True,max_features=700)),
                      ("tf_idf_ngra_3_ch",TfidfVectorizer(min_df=1,ngram_range=(1, 3),binary=True,max_features=700,analyzer='char')),
                      ("tf_idf_ngra_2_ch",TfidfVectorizer(min_df=1,ngram_range=(1, 2),binary=True,max_features=700,analyzer='char')),
                      ("tf_idf_ngra_1_ch",TfidfVectorizer(min_df=1,ngram_range=(1, 1),binary=True,max_features=700,analyzer='char')),
                      ("tf_idf_ngra_5_ch",TfidfVectorizer(min_df=1,ngram_range=(1, 5),binary=True,max_features=700,analyzer='char')),
                      ("tf_idf_ngra_7_ch", TfidfVectorizer(min_df=1,ngram_range=(1, 7),binary=True,max_features=700,analyzer='char')),
                      ("cnt_vec_ngra_3", CountVectorizer(min_df=1,ngram_range=(1, 3),binary=True,max_features=700)),
                      ("cnt_vec_ngra_5", CountVectorizer(min_df=1,ngram_range=(1, 5),binary=True,max_features=700))
                                      
                                      
                                      ])
      self._feat_union = union

  def use_recomended_feature_vec(self,char_ngram_range,word_ngram_range):
      word_vectorizer = TfidfVectorizer(lowercase=False,
                                          ngram_range=word_ngram_range,
                                          analyzer='word',
                                          tokenizer=lambda x: x.split(' '))
      char_vectorizer = TfidfVectorizer(lowercase=False,
                                          ngram_range=char_ngram_range,
                                          analyzer='char',
                                          tokenizer=lambda x: x.split(' '))
      self._feat_union = FeatureUnion([('wordgrams', word_vectorizer),
                                               ('chargrams', char_vectorizer)])
        


  

  def train(self, X_train,
              y_train,
              use_defult_fearures=False,
              char_ngram_range=(1, 3),
              word_ngram_range=(1, 1),
              n_jobs=None):
        """Trains the model on a given data set.
        Args:
            X_train (:obj:`np array or pandas series`, optional): loaded training data.
               
            y_train (:obj:`np array or pandas series`, optional): loaded labels for training.

            use_defult_fearures (:obj:'bool', optional): Use recommended feature vector 


            char_ngram_range (:obj:`tuple`, optional): The n-gram ranges to
                consider in the character-based language models.
                Defaults to (1, 3).
            word_ngram_range (:obj:`tuple`, optional): The n-gram ranges to
                consider in the word-based language models.
                Defaults to (1, 1).
            n_jobs (:obj:`int`, optional): The number of parallel jobs to use
                for computation. If None, then only 1 job is used.
                If -1 then all processors are used. Defaults to None.
        """

       

        
       

        # Build and train extra classifier
        if use_defult_fearures:
          self.use_defult_fearure_vec()
        else:
          self.use_recomended_feature_vec(char_ngram_range,word_ngram_range)   

        x_prepared = self._prepare_sentences(X_train)    
       
       

        self._classifier = CalibratedClassifierCV(LinearSVC(C=0.01, class_weight='balanced', penalty='l2'))
        self._classifier.fit(x_prepared, y_train)
        self._is_trained = True

  def eval(self,  X_eval,
              y_eval, data_set='DEV'):
        """Evaluate the trained model on a given data set.
        Args:
            X_eval (:obj:`np array or pandas series`, optional): loaded data for evaluation.

            y_eval (:obj:`np array or pandas series`, optional): loaded labels for evaluation.

            data_set (:obj:`str`, optional): Name of the provided data set to
                use. This is ignored if data_path is not None. Can be either
                'VALIDATION' or 'TEST'. Defaults to 'VALIDATION'.
        Returns:
            :obj:`dict`: A dictionary mapping an evaluation metric to its
            computed value. The metrics used are accuracy, f1_micro, f1_macro,
            recall_micro, recall_macro, precision_micro and precision_macro.
        """

        if not self._is_trained:
            raise UntrainedModelError(
                'Can\'t evaluate an untrained model.')

       

        # # prepare data
        # eval_data = self._prepare_sentences(X_eval)
        

        # Generate predictions
        did_pred = self.predict(X_eval)
        did_pred = [d.top for d in did_pred]
       

        # Get scores
        scores = {
            'Sentiment': {
                'accuracy': accuracy_score(y_eval, did_pred),
                'f1_macro': f1_score(y_eval, did_pred,
                                     average='macro'),
                'recall_macro': recall_score(y_eval, did_pred,
                                             average='macro'),
                'precision_macro': precision_score(y_eval,
                                                   did_pred,
                                                   average='macro')
            }
        }

        return scores    

  def predict(self, sentences, output='label'):
        """Predict the sentiment  probability scores for a given list of
        sentences.
        Args:
            sentences (:obj:`list` of :obj:`str`): The list of sentences.
            output (:obj:`str`): The output label type. Possible values are
                'postive', 'neagtive', 'neutral'.
        Returns:
            :obj:`list` of :obj:`DIDPred`: A list of prediction results,
            each corresponding to its respective sentence.
        """

        if not self._is_trained:
            raise UntrainedModelError(
                'Can\'t predict with an untrained model.')

        
        if isinstance(sentences, str):
          sentences=pd.Series(sentences)
        x_prepared = self._prepare_sentences(sentences)
        predicted_scores = self._classifier.predict_proba(x_prepared)
        convert = lambda x: x

        result = collections.deque()
        for scores in predicted_scores:
            score_tups = list(zip(self._labels_sorted, scores))
            predicted_sentiment = max(score_tups, key=lambda x: x[1])[0]
            sentiment_scores = dict(score_tups)
            result.append(convert(DIDPred(predicted_sentiment, sentiment_scores)))

        return list(result)

  def save_model(self,path):
      """Save  the model on a given data set.
        Args:
            Path (:obj:`str`): Path where you want to save the model.
               
           
        """
      joblib.dump(self._classifier, path+'/svm_model.sav')
  def save_feature_vec(self,path):
    """Save  the feature vector on a given data set.
      Args:
          Path (:obj:`str`): Path where you want to save the feature vector .
              
          
      """
    
    # torch.save(obj=self._feat_union,
    #     f=path+'/feat_vec.pkl',
    #     pickle_module=dill
    # )
    dill.dump(self._feat_union, open( path+"/feat_vec.p", "wb" ))
    # pickle.dump(self._feat_union.get_params(), open( path+"/feat_vec.p", "wb" ) )
    # joblib.dump(pipeline, 'filename.pkl', compress = 1)

    # joblib.dump(self._feat_union.get_params(), path+'/feat_vec.pkl', compress = 1)

class UntrainedModelError(SentimentIdentificationML):
    """Error thrown when attempting to use an untrained sentiment classifier
    instance.
    """

    def __init__(self, msg):
        DialectIdError.__init__(self, msg)

class DIDPred(collections.namedtuple('DIDPred', ['top', 'scores'])):
    """A named tuple containing sentiment ID prediction results.
    Attributes:
        top (:obj:`str`): The sentiment label with the highest score. See
            :ref:`sentimentid_labels` for a list of output labels.
        scores (:obj:`dict`): A dictionary mapping each sentiment label to it's
            computed score.
    """

MODEL_PATH='/content/drive/MyDrive/Omdena_sentiment/Saved_models/Production/svm_model.sav'
FEATURE_VEC_PATH='/content/drive/MyDrive/Omdena_sentiment/Saved_models/Production/feat_vec.p'

class SentimentIdentificationMLInfereing(object):
  """A class for running the sentiment classification based on pretrained ML model
     
    Args:
        labels (:obj:`set` of :obj:`str`, optional): The set of dialect labels
            used in the training data in the main model.
            If None, the default labels are used.
            Defaults to None.

        training_model_path (:obj:`str`, optional): Path of training model to be used for inference,
        If none, use defult model for this libaray
        feature_vector_path (:obj:`str`, optional): Path of feature vector to be used for preparing data,
        If none, use defult model for this libaray
       
    """
  def __init__(self, labels=None,training_model_path=None,feature_vector_path=None,
                 ):
        if labels is None:
            labels = _DEFAULT_LABELS
        self._labels_sorted = sorted(labels)
        if feature_vector_path is None:
          self._feat_union=dill.load(open(FEATURE_VEC_PATH, 'rb'))
          # self._feat_union_params=  joblib.load(FEATURE_VEC_PATH)
          # self._feat_union=FeatureUnion()
          # self._feat_union.set_params(self._feat_union_params)
        else:
          self._feat_union=  dill.load(open(feature_vector_path, 'rb'))
        if training_model_path is None:
          self._classifier=  joblib.load(MODEL_PATH)
        else:
          self._classifier=  joblib.load(training_model_path)

  def _apply_preprocessing(self,text):

      # ref: https://github.com/bakrianoo/aravec
      search = ["أ","إ","آ","ة","_","-","/",".","،"," و "," يا ",'"',"ـ","'","ى",
                "\\",'\n', '\t','&quot;','?','؟','!']
      replace = ["ا","ا","ا","ه"," "," ","","",""," و"," يا",
                "","","","ي","",' ', ' ',' ',' ? ',' ؟ ', ' ! ']
      
      tashkeel = re.compile(r'[\u0617-\u061A\u064B-\u0652]')
      text = re.sub(tashkeel,"", text)
      
      longation = re.compile(r'(.)\1+')
      subst = r"\1\1"
      text = re.sub(longation, subst, text)
      
      text = re.sub(r"[^\w\s]", '', text)
      text = re.sub(r"[a-zA-Z]", '', text)
      text = re.sub(r"\d+", ' ', text)
      text = re.sub(r"\n+", ' ', text)
      text = re.sub(r"\t+", ' ', text)
      text = re.sub(r"\r+", ' ', text)
      text = re.sub(r"\s+", ' ', text)
      text = text.replace('وو', 'و')
      text = text.replace('يي', 'ي')
      text = text.replace('اا', 'ا')
      
      for i in range(0, len(search)):
          text = text.replace(search[i], replace[i])
      
      text = text.strip()


      
      return text


  def _prepare_sentences(self, sentences):

        
      cleaned_sent=  sentences.apply(self._apply_preprocessing)             
      sent_array = np.array(cleaned_sent)       
      x_trans = self._feat_union.transform(sent_array)        
      return x_trans


  def eval(self,  X_eval,
              y_eval, data_set='DEV'):
        """Evaluate the trained model on a given data set.
        Args:
            X_eval (:obj:`np array or pandas series`, optional): loaded data for evaluation.

            y_eval (:obj:`np array or pandas series`, optional): loaded labels for evaluation.

            data_set (:obj:`str`, optional): Name of the provided data set to
                use. This is ignored if data_path is not None. Can be either
                'VALIDATION' or 'TEST'. Defaults to 'VALIDATION'.
        Returns:
            :obj:`dict`: A dictionary mapping an evaluation metric to its
            computed value. The metrics used are accuracy, f1_micro, f1_macro,
            recall_micro, recall_macro, precision_micro and precision_macro.
        """

        

        # # prepare data
        # eval_data = self._prepare_sentences(X_eval)
        

        # Generate predictions
        did_pred = self.predict(X_eval)
        did_pred = [d.top for d in did_pred]
        
       

        # Get scores
        scores = {
            'Sentiment': {
                'accuracy': accuracy_score(y_eval, did_pred),
                'f1_macro': f1_score(y_eval, did_pred,
                                     average='macro'),
                'recall_macro': recall_score(y_eval, did_pred,
                                             average='macro'),
                'precision_macro': precision_score(y_eval,
                                                   did_pred,
                                                   average='macro')
            }
        }

        return scores    

  def predict(self, sentences, output='label'):
        """Predict the sentiment  probability scores for a given list of
        sentences.
        Args:
            sentences (:obj:`list` of :obj:`str`): The list of sentences.
            output (:obj:`str`): The output label type. Possible values are
                'postive', 'neagtive', 'neutral'.
        Returns:
            :obj:`list` of :obj:`DIDPred`: A list of prediction results,
            each corresponding to its respective sentence.
        """
        if isinstance(sentences, str):
          sentences=pd.Series(sentences)
        x_prepared = self._prepare_sentences(sentences)
        predicted_scores = self._classifier.predict_proba(x_prepared)
        convert = lambda x: x

        result = collections.deque()
        for scores in predicted_scores:
            score_tups = list(zip(self._labels_sorted, scores))
            predicted_sentiment = max(score_tups, key=lambda x: x[1])[0]
            sentiment_scores = dict(score_tups)
            result.append(convert(DIDPred(predicted_sentiment, sentiment_scores)))

        return list(result)

"""# Testing Production Code"""

df=pd.read_csv('/content/drive/MyDrive/Omdena_sentiment/Dataset/final_text.csv')

df.head(5)

msk = np.random.rand(len(df)) < 0.7
train = df[msk]
test = df[~msk]

msk = np.random.rand(len(train)) < 0.8
train_new = train[msk]
valid = train[~msk]

train_new.head()

len(train_new)

valid.head()

test.head()

labels_numeric=[0,1,2]

train_new.dropna(inplace=True)

test.dropna(inplace=True)

sentimenclassifier=SentimentIdentificationML(labels_numeric)

sentimenclassifier.train(train_new['final'],train_new['label'],use_defult_fearures=False)

valid.dropna(inplace=True)

valid.head()

sentimenclassifier.eval(valid['final'],valid['label'])

test.iloc[0]['final']

sentimenclassifier.predict('صفاء الهاشم سيده كويتيه المراه الوحيده حاليا مجلس الامه الكويتي مدافعه شرسه حقوق المراه وحق المواطن الكويتي وتمتلك عقليه اقتصاديه مميزه اختيرت ضمن سيده عربيه مؤثره مجتمعها')

sentimenclassifier.save_feature_vec('/content/drive/MyDrive/Omdena_sentiment/Saved_models/Production')

sentimenclassifier.save_model('/content/drive/MyDrive/Omdena_sentiment/Saved_models/Production')

InfereSentiment=SentimentIdentificationMLInfereing(labels_numeric)

InfereSentiment.eval(test['final'],test['label'])