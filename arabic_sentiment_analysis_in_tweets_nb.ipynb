{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "arabic-sentiment-analysis-in-tweets-nb.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohamadElnomrossie/Omdena-seniment-analysis/blob/rest_ML_models(Naive_Bayes)/arabic_sentiment_analysis_in_tweets_nb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXcCNEw2HIaa"
      },
      "source": [
        "# Arabic Sentiment Analysis in tweets using Naive Bayes Machine learning Algorithm \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnkEK8ZiHIag",
        "outputId": "74e7680e-431d-42bc-caed-df416cd0d104"
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import nltk\n",
        "from nltk import NaiveBayesClassifier\n",
        "from nltk.metrics.scores import f_measure, precision, recall\n",
        "import collections\n",
        "\n",
        "\n",
        "# Input data files are available in the \"input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "import os\n",
        "path= \"/content/input/\"\n",
        "for filename in os.listdir(path):\n",
        "    print(filename)\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_Arabic_tweets_negative_20190413.tsv\n",
            "train_Arabic_tweets_negative_20190413.tsv\n",
            "test_Arabic_tweets_positive_20190413.tsv\n",
            "train_Arabic_tweets_positive_20190413.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78nl1sHQHIak"
      },
      "source": [
        "# define functions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1AtFMfwHIak"
      },
      "source": [
        "import re\n",
        "from itertools import islice\n",
        "\n",
        "def load_tsv(data_file, n):\n",
        "    data_features = list()\n",
        "    data = list()\n",
        "    infile = open(data_file, encoding='utf-8')\n",
        "    for line in infile:\n",
        "        if not line.strip():\n",
        "            continue\n",
        "        label, text = line.split('\\t')\n",
        "        text_features = process_text(text, n)\n",
        "        if text_features:\n",
        "            data_features += text_features\n",
        "            data.append((text_features, label))\n",
        "    return data, data_features\n",
        "\n",
        "def process_text(text, n=1,\n",
        "                 remove_vowel_marks=False,\n",
        "                 remove_repeated_chars=False,\n",
        "                 ):\n",
        "    clean_text = text\n",
        "    if remove_vowel_marks:\n",
        "        clean_text = remove_diacritics(clean_text)\n",
        "    if remove_repeated_chars:\n",
        "        clean_text = remove_repeating_char(clean_text)\n",
        "\n",
        "    if n == 1:\n",
        "        return clean_text.split()\n",
        "    else:\n",
        "        tokens = clean_text.split()\n",
        "        grams = tokens\n",
        "        for i in range(2, n + 1):\n",
        "            grams += [  ' '.join(g) for g in list(window(tokens, i))  ]\n",
        "        return grams\n",
        "\n",
        "\n",
        "\n",
        "def window(words_seq, n):\n",
        "    \"\"\"Returns a sliding window (of width n) over data from the iterable\"\"\"\n",
        "    \"   s -> (s0,s1,...s[n-1]), (s1,s2,...,sn), ...                   \"\n",
        "    it = iter(words_seq)\n",
        "    result = tuple(islice(it, n))\n",
        "    if len(result) == n:\n",
        "        yield result\n",
        "    for elem in it:\n",
        "        result = result[1:] + (elem,)\n",
        "        yield result\n",
        "\n",
        "\n",
        "def remove_repeating_char(text):\n",
        "    # return re.sub(r'(.)\\1+', r'\\1', text)     # keep only 1 repeat\n",
        "    return re.sub(r'(.)\\1+', r'\\1\\1', text)  # keep 2 repeat\n",
        "\n",
        "def document_features(document, corpus_features):\n",
        "    document_words = set(document)\n",
        "    features = {}\n",
        "    for word in corpus_features:\n",
        "        features['has({})'.format(word)] = (word in document_words)\n",
        "    return features"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3bYV8HcHIam"
      },
      "source": [
        "# Load corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjMTF61EHIan",
        "outputId": "5f2053c9-0405-4d3c-d6f5-139273b0ee01"
      },
      "source": [
        "pos_train_file = 'input/train_Arabic_tweets_positive_20190413.tsv'\n",
        "neg_train_file = 'input/train_Arabic_tweets_negative_20190413.tsv'\n",
        "\n",
        "pos_test_file = 'input/test_Arabic_tweets_positive_20190413.tsv'\n",
        "neg_test_file = 'input/test_Arabic_tweets_negative_20190413.tsv'\n",
        "print('data files')\n",
        "print('train file (pos)', pos_train_file)\n",
        "print('train file (neg)', neg_train_file)\n",
        "print('test file (pos)', pos_test_file)\n",
        "print('test file (neg)', neg_test_file)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data files\n",
            "train file (pos) input/train_Arabic_tweets_positive_20190413.tsv\n",
            "train file (neg) input/train_Arabic_tweets_negative_20190413.tsv\n",
            "test file (pos) input/test_Arabic_tweets_positive_20190413.tsv\n",
            "test file (neg) input/test_Arabic_tweets_negative_20190413.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MyBYwNHHIao"
      },
      "source": [
        "# Parameters (ngrams)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLk-BF8YHIap",
        "outputId": "1d31d3b6-94b1-4ad9-aebc-6aaa3461c8ad"
      },
      "source": [
        "print('parameters')\n",
        "n = 1\n",
        "print('n grams:', n)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "parameters\n",
            "n grams: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUfnhYYBHIaq"
      },
      "source": [
        "# loading train data .... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWPG6NvvHIaq",
        "outputId": "1d082eb9-955b-4ca8-9dc5-0549d39db747"
      },
      "source": [
        "print('loading train data ....')\n",
        "pos_train_data, pos_train_feat = load_tsv(pos_train_file, n)\n",
        "neg_train_data, neg_train_feat = load_tsv(neg_train_file, n)\n",
        "print('loading test data ....')\n",
        "pos_test_data, pos_test_feat = load_tsv(pos_test_file, n)\n",
        "neg_test_data, neg_test_feat = load_tsv(neg_test_file, n)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading train data ....\n",
            "loading test data ....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV-KVZ7bHIar"
      },
      "source": [
        "# Training data information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hD_EspLOHIas",
        "outputId": "30aaedca-e1d4-4c1f-f68c-1868bbddc6cc"
      },
      "source": [
        "print('train data info')\n",
        "train_data = pos_train_data + neg_train_data\n",
        "print('train data size', len(train_data))\n",
        "print('# of positive', len(pos_train_data))\n",
        "print('# of negative', len(neg_train_data))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train data info\n",
            "train data size 47000\n",
            "# of positive 23879\n",
            "# of negative 23121\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6wqvfopHIat"
      },
      "source": [
        "# Sample training data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gU0QJvDZHIat",
        "outputId": "b5843c17-82c2-41b5-9316-2a4db1f6cbe5"
      },
      "source": [
        "import random\n",
        "sample_size = 100\n",
        "print('{} random tweets .... '.format(sample_size))\n",
        "for s in random.sample(train_data, sample_size):\n",
        "    print(s)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 random tweets .... \n",
            "(['صباح', 'الخير', '..🌻', 'ينام', 'العبد', 'على', 'أمر', 'قد', 'يئس', 'منه', 'و', 'يستيقظ', 'على', 'انفراجه', '،', 'دع', 'الخالق', 'يجبر', 'كسر', 'قلبك', 'و', 'اجعل', 'همومك', 'وراء', 'ظه…'], 'pos')\n",
            "(['الحمد', 'لله', 'على', 'الفوز', 'و', 'الصدارة', 'المؤقته', 'بتكاتف', 'الجميع', 'و', 'عدم', 'التفريط', 'في', 'اي', 'نقطه', 'بإذن', 'الله', 'نحقق', 'المطلوب', '💙'], 'pos')\n",
            "(['بدون', 'أخيك', 'ستأكلك', 'الضباع', 'حتى', 'لو', 'كنت', 'أسدا', '..', 'فلا', 'تفرط', 'ذات', 'يوم', 'بأخيك', '💔', '✨'], 'neg')\n",
            "(['سبحان', 'الله', '💙'], 'pos')\n",
            "(['عادي', 'شاعرها', 'أو', 'شاعرك', '🤔', '#Malak'], 'neg')\n",
            "(['كل', 'المتابعين', 'نايمين', 'لا', 'احساس', 'ولا', 'ضمير', 'ضايفه', 'محلات', 'أنا', '😜', 'ما', 'تفتح', 'الا', 'عقب', 'العشر', '😳'], 'neg')\n",
            "(['منتظرتك', 'تنامين', 'نامي', '😡'], 'neg')\n",
            "(['مالتي', 'ماكا👉', 'منشط', 'طبيعي🐝', '💯💯', 'وخالى', 'من', 'اي', 'كيماويات', 'وبديل', 'للفياجرا', '👋', 'يعالج', 'سرعة', 'القذف', 'المبكر🌿', 'يحسن', 'من…'], 'pos')\n",
            "(['الآرمي', 'مبدعين', '😰', 'ناويين', 'يركبونها', 'على', 'كل', 'انمي', 'وهي', 'تركب', 'على', 'كل', 'انمي', 'تجنن', 'الأغنية🤧💕'], 'neg')\n",
            "(['📌|', 'مشاريع', 'مقرر', 'النظم', 'الذكية', 'لطالبات', 'قسم', 'علوم', 'الحاسب', 'لعام', 'إشراف', 'د.', 'الشيماء', 'ندا', 'بكلية', 'علوم', 'وهندسة', 'الحاسب', 'الآلي…'], 'pos')\n",
            "(['روابط', 'البث', 'المباشر', 'لكامل', 'مباريات', 'البطولة:', '.', '.', '.…'], 'pos')\n",
            "(['📌', 'عن', 'أبﮯ', 'هريرة', 'رضﮯ', 'الله', 'عنہ', '،', 'قال', ':', 'قال', 'رسول', 'الله', 'ﷺ', ':', '❐', 'إن', 'الله', 'كتب', 'على', 'ابن', 'آدم', 'حظہ', 'من…'], 'neg')\n",
            "(['والله', 'الشوارع', 'بهرجة', 'الف', '😂'], 'pos')\n",
            "(['🎥', 'المهمة', 'الأولى', 'في', '\"جدة\"', '✔💪🏼', '💙', '#الهلال', '#فيديو_الهلال'], 'pos')\n",
            "(['\"تلاتين', 'سنة', 'بترقص', '..', 'الليلة', 'رقصتنا\"', 'أنا', 'ببكي', '😭', 'دي', 'حلاوة', 'شديدة', 'باخ'], 'neg')\n",
            "(['ماكل', 'خاطر', 'نبي', 'نتبع', '..', 'مراعاته', 'بعض', 'الخواطر', 'حرام', 'انك', 'تراعيها', '،', 'واللي', 'يقدمك', 'عن', 'كل', '..', 'اهتماماته', 'حاول', 'تقدم', 'له', 'الدني…'], 'neg')\n",
            "(['..', 'حي', 'الله', 'اللي', 'يبادلني', 'معزه', 'واحترام', 'شيخ', 'الرياجيل', 'خيي', 'سيد', 'هذا', 'و', 'ذا', '.', '#ريوف', '🌹'], 'pos')\n",
            "(['لا', 'تجاملني', 'بأسف', 'خل', 'أسف', 'للجديد', '💔'], 'neg')\n",
            "(['أحسنت', 'يابو', 'هاجر', '😍'], 'pos')\n",
            "(['ياريت', 'كل', 'حد', 'يشوف', 'الكلام', 'ده،', 'يدعي', 'لماما', 'ربنا', 'يشفيها', 'ويقومها', 'بالسلامه', '💔'], 'neg')\n",
            "(['الحمد', 'لله', '..', 'كانت', 'مباراه', 'صعبه', 'ولكن', 'بتوفيق', 'الله', 'ثم', 'دعمكم', 'تجاوزنا', 'المباراه', '💙'], 'pos')\n",
            "(['منشور', 'رمضاني', 'للبنات', 'بنات', 'اجانه', 'رمضان', '🌜', 'حاولن', 'توسعن', 'ملابسجن', 'وخلنه', '👏', 'انصوم', 'مثل', 'الاوادم', '🙄', 'لخاطر', 'ربجن', '😕'], 'neg')\n",
            "(['الصراحة', 'مالقيت', 'تعليق', 'مناسب', 'معقولة', '😳', 'بارك', 'الله', 'في', 'الشيخ', 'الدكتور', '#عثمان_الخميس', 'طرشوا', 'حق', '#الزفوت'], 'neg')\n",
            "(['المرأة', 'التي', 'تستيقظ', 'قبل', 'الفجر', 'ثم', 'تصلي', 'وتعجن', 'وتخبز', 'وتحضر', 'الفطور', 'لزوجها', 'ثم', 'ترافقه', 'إلى', 'الباب', 'وتدعو', 'له', 'بالرزق', '!', 'في', 'أي', 'كوكب', 'نجدها', '!؟', '🤔'], 'neg')\n",
            "(['كم', 'سرني', 'وأسعدني', 'مرور', 'أخوين', 'كريمين', ':(', 'خالد', 'البسام', '،', 'أنا', 'سعودي', ')أسعدكما', 'الله', 'بطاعته', 'و', 'شرفكما', 'برضاه', 'و', 'محبته', 'و', 'أكرمكما…'], 'neg')\n",
            "(['ليت', 'وجهك', 'قبالي', 'كل', 'ما', 'أمطرت', ':('], 'neg')\n",
            "(['وظنك', 'يمر', 'صبح', 'ماحبك', 'فيه', 'صباحي', 'انت', 'واحبك', 'جدا', '💛'], 'pos')\n",
            "(['\"يوما', 'ما', 'ستبدو', 'فخورا', 'بكل', 'الصعاب', 'التي', 'واجهت', 'النصر', '،', 'بكل', 'لحظة', 'خوف،', 'توتر،', 'قلق،', 'سهر،', 'ستبدو', 'فخورا', 'جدا', 'بعبورك', 'نحو', 'القمه', '.', '💛'], 'pos')\n",
            "(['قصدكم', 'فيه', 'دوام', 'اليوم', 'عندكم', '😯'], 'neg')\n",
            "(['الله', 'على', 'الشفافية', 'وعدم', 'المجاملة', '😂'], 'pos')\n",
            "(['اوه', 'شيت', 'صحت', '💔'], 'neg')\n",
            "(['_', 'راسي', 'عنيد', 'ولا', 'كل', 'شي', 'بيرضيني', 'طبعي', 'مكابر', 'وكايد', 'واعترف', 'قاسيه', '☺️', '#الزين'], 'pos')\n",
            "(['افا', 'هذي', 'اخرتها', 'نردد', 'نشيد', '#الاتحاد', '؟', 'طول', 'عمرنا', 'نهايط', 'ونقول', '#الهلال', 'مايهمه', 'أحد', 'وفوق', 'الجميع', 'والحين', 'ندور', 'الفزعة', 'من', 'فريق…'], 'neg')\n",
            "(['+١مبطيقش', 'الرغى', 'الكتير', 'وكذا', 'حد', 'يتكلم', 'فنفس', 'الوقت', '😑'], 'neg')\n",
            "(['نقول', 'يارب', 'بس', 'الوان', 'القلبين', 'اللي', 'حاطهم', 'معطي', 'الوان', 'النصر', '🤔'], 'neg')\n",
            "(['ارجووكم', 'اتوجهو', 'للقياده', 'في', 'تمثيليه', 'بتاعت', 'ضرب', 'رصاص', 'في', 'الهوا', 'وبيقولو', 'ديل', 'قناصين', 'عشان', 'الناس', 'تخاف', 'وبيشيلو', 'في', 'المتاريس', 'ح…'], 'neg')\n",
            "(['ما', 'رح', 'اتخلى', 'عنك', '❤', 'حتى', 'لو', 'بدي', 'ازعل', 'واتوجع', 'منك', '💔', 'لانو', 'الزعل', 'معك', 'احلا', 'من', 'الفرح', 'مع', 'غيرك', '😍', '💙', 'K', '💙'], 'neg')\n",
            "(['☁̼', 'ོོ', '⠀ོ⠀', 'إذا', 'مررت', 'من', 'هنا', 'فصلي', 'على', 'الرسول', 'محمد', 'ﷺ', '🍂'], 'pos')\n",
            "(['تويتر', 'ماله', '😡', 'ايه', 'اللى', 'ملخبطه', '🤔', 'بيلغى', 'متابعين', 'على', 'حسابى', 'بالعشرة', 'وأكثر', 'هو', 'ده', 'طبيعى', 'ياتويتر', 'ولا', 'بتزهقنا', 'علشان', 'ماندخلش', '😏'], 'neg')\n",
            "(['ان', 'جيتوا', 'للحق', 'أنا', 'اللي', 'سقفت', 'للقرد', 'وزعلت', 'لما', 'اتنطط', '🌚'], 'neg')\n",
            "(['.لإﺂ', 'تغرگ', 'ضحگتي', 'ؤتقول❌', '#مرتإﺂحﮩ😌', ';رگز', 'بإﺂلضحگهہ❌', 'تسمع', 'صؤت…', '#الونين', '😧'], 'neg')\n",
            "(['#ياقريب', 'في', 'القلب', 'أمنيات', 'فحققها', '#يارحيم', '#يارحمن', '#يالطيف', '#يامجيب', '#ياسميع', '#ياعليم', '#ياحي', '#ياقيوم', '#ياكريم', '#ياقدير…'], 'pos')\n",
            "(['لو', 'عمري', 'ڪلهۂ', 'وجع', 'منڪ', 'ماترڪتك', 'أنا', 'بحلوڪ', 'ومرڪ', 'عشقتك', '{', '↝', '❤', '#S', '❤', '↜', '}', '،˛', '…'], 'pos')\n",
            "(['جملة', 'الرئيس', 'السابق', 'عمر', 'البشير', 'دي', 'زابطه', 'كيف', '😂'], 'pos')\n",
            "(['أستشعر', 'بك', 'داخل', 'قلبي', 'بكل', 'نبضة', '،،', 'فأجدني', 'حاضنا', 'نفسي', 'شعورا', 'بك', '..', '💜'], 'pos')\n",
            "(['#الهلال_الاهلي', 'والله', 'فاتني', 'كنت', 'طالعه', '😭', 'بس', 'الحمدلله', 'انهم', 'فرحونا', 'الليله', 'و', 'الف', 'مليون', 'مبروك', 'للزعيم', 'وجمهوره', '💙💙'], 'neg')\n",
            "(['ما', 'رح', 'اتخلى', 'عنك', '❤', 'حتى', 'لو', 'بدي', 'ازعل', 'واتوجع', 'منك', '💔', 'لانو', 'الزعل', 'معك', 'احلا', 'من', 'الفرح', 'مع', 'غيرك', '😍', '💙', 'K', '💙'], 'neg')\n",
            "(['اذا', 'الفيفا', 'عارف', 'هالشي', 'غلط', 'مش', 'المفروض', 'يتصرف', 'ويتخذ', 'اجراءه', 'ولا', 'الاتحاد', 'السعودي', 'اقوى', 'منه', 'ولا', 'هاي', 'الشوشرا…'], 'neg')\n",
            "(['صباحك', 'سعيد', 'جميل', 'رقيق'], 'pos')\n",
            "(['مليون', 'ريال', 'سعودي', 'سنويا', '😅'], 'pos')\n",
            "(['بقعد', 'خمس', 'دقايق', 'أقلب', 'المفاتيح', 'فى', 'إيدى', 'و', 'أنور', 'بالموبايل', 'لحد', 'ما', 'أجيب', 'المفتاح', 'الصح', '..', 'وأول', 'ما', 'اوصل', 'للباب', '..', 'تقوم', 'المفاتيح', 'تقع', 'منى', '😡'], 'neg')\n",
            "(['كيف', 'عرفت', '😬'], 'neg')\n",
            "(['وصافة', 'بس', '😂'], 'pos')\n",
            "(['عندو', 'اختبار', 'ولا', 'شنوو', '🤔'], 'neg')\n",
            "(['لو', 'مثلا', 'قمت', 'على', 'موقف', 'جذي', '🤔', 'راح', 'يكون', 'آخر', 'يوم', 'في', 'حيات', 'الي', 'يسويها', '+', 'مافي', 'حكومة', 'تقدر', 'تفكه', 'من', 'يدي', '🤔', 'بس'], 'neg')\n",
            "(['بمناسبة', 'فوز', 'الهلال', '..', '💙', 'سحب', 'على', 'آيفون', 'XR📱', 'رتويت', 'وتابع', '-', 'السحب', 'بعد', 'ساعة', 'موثق', 'بالفديو', '💪'], 'pos')\n",
            "(['شو', 'قصدك', '😒'], 'neg')\n",
            "(['حقير', 'الشوق', 'وهو', 'فعلا', 'حقير', '💔'], 'neg')\n",
            "(['ستيج', 'The', 'story', 'مرره', 'حلو', 'الاغنيه', 'هاديه', 'ورايقه', 'مع', 'اصواتهم', 'والراب', 'والاوتفيت', 'حبييت', 'حبييت', 'والرقصه', 'عجببتني', 'بعد', 'اعطيها…'], 'neg')\n",
            "(['لابوكم', 'لبو', 'هياطكم', 'ياطواقي', 'ولكم', 'وجه', 'تتكلمو', 'على', 'تاج', 'راسكم', 'الملكي', 'ياعمي', 'توكل', 'أنت', 'وفريقك', 'الفاسد', '✋'], 'neg')\n",
            "(['🔻', 'متى', 'تكتمل', 'طبخة', 'إستكمال', 'الكابينة', 'الحكومية', '!!؟', '.', 'ندعو', '#الشعب', '#السوداني', 'الشقيق', 'بالضغط', 'على', '#الحكومة', '#العراقية', 'لإستكم…'], 'neg')\n",
            "(['اي', 'والله', 'من', 'جد', 'التحديث', 'الاخير', 'مخيس', 'بمعنى', 'الكلمة', '😑', 'صفحة', 'الردود', 'كأنه', 'حوض', 'مطبخ', 'مليان', 'صحون', 'عزيمة', 'ما', 'انغسلت', 'من', 'أمس'], 'neg')\n",
            "(['ياخديجة', 'كنت', 'فقيرا', 'فأغناني', 'الله', 'بك”', '-', 'محمد', 'صل', 'الله', 'عليه', 'وسلم', '💙'], 'pos')\n",
            "(['هنا', 'أتوقف', '..', 'ولن', 'أتحدث', 'عن', 'البقية', '..', 'إلا', 'إذا', 'وصلت', 'التغريدة', 'آلاف', 'ريتويت', '🙈', '..', 'وهذا', 'مستحيل', '..', 'عشان', 'شذيه', 'نلقاكم', 'على', 'خير', '💕🌺'], 'neg')\n",
            "(['الحياة', 'مجرد', 'صفحات', 'نحن', 'من', 'نروي', 'القصص', 'فيها..', '🍁', '#تايه', '💔', '#اجمل_احساس_للدعم'], 'pos')\n",
            "(['قرية', 'الوافي', 'عندام', 'التابعة', 'لولاية', 'المضيبي', 'وليست', 'نيابة', 'سناو', '🌚'], 'neg')\n",
            "(['\"', 'الفترة', 'دي', 'صعبه', 'علي', 'كل', 'الناس', 'بطريقة', 'غريبه', 'جدا', '\"', 'كلنا', 'عندنا', 'ابتلاءات', 'بإختلاف', 'التفاصيل..ربنا', 'يهونها', 'علينا', 'جميعا', '💔'], 'neg')\n",
            "(['لا', 'تكسر', 'قلبك', 'لأجل', 'شخص', 'لا', 'يستحقك', '!🙂'], 'pos')\n",
            "(['انو', 'اقعد', 'اشتغل', 'على', 'مشاريعي', '😑'], 'neg')\n",
            "(['باليز', 'يا', 'انا', 'اذا', 'بتدخلين', 'تويتر', 'حاولي', 'ماتدخلين', 'حسابك', 'الثاني', 'خلك', 'هنا', '،ولا', 'تجيبين', 'العيد', '😭'], 'neg')\n",
            "(['#صباح_الخير', 'وصلني', 'هذا', 'من', 'احد', 'الشباب', 'مبررا', 'موقفه', 'او', 'رغبته', 'بالتعدد', 'شي', 'هذا', 'حقك', '✋', '#لكن', 'قوله', 'ان', 'كل', 'ما', 'زاد', 'عدد…'], 'neg')\n",
            "(['اللهم', 'ارزقه', 'الخشونه', 'صدقني', 'انت', 'محتاج', 'لك', 'شمس', 'تقعد', 'فيها', 'عشان', 'تنشف', '😒'], 'neg')\n",
            "(['جمهورنا', ':', 'المتبعثر', 'المكسور', '!', 'ياما', ':', 'عطا', '..', 'ولا', 'أخذ', ':', 'حاجة', '!', \"'\", 'ياما', ':', 'وفا', '..', 'ولا', 'عليه', '\"', 'قصور', '\"', 'ياما', 'انجرح', '..', 'ولا', 'لق…'], 'neg')\n",
            "(['#الفيفا_يرد_احتجاج_الهلال_صحيح', 'صراحه', 'مو', 'شايفه', 'التغريده', 'بحسابهم', '😂', 'من', 'وين', 'جايبين', 'هالقرار', 'انتم', '!!😅'], 'pos')\n",
            "(['سبحان', 'الله', '🌷', 'والحمدلله', '🍃', 'ولا', 'إله', 'إلا', 'الله🌺', 'والله', 'أكبر', '🍂', 'ولا', 'حول', 'ولا', 'قوة', 'إلا', 'بالله', '🌼'], 'pos')\n",
            "(['افتر', 'راسي', 'مع', 'تصويرك', 'حسبتك', 'تصورين', 'طيور', 'فالسماء', '😂'], 'pos')\n",
            "(['\"ومن', 'أحياها', 'فكأنما', 'أحيا', 'الناس', 'جميعا\"', 'اخوكم', 'بندر', 'جلاله', 'بحاجة', '\\u2067#تبرع\\u2069', '\\u2067#كلى\\u2069', 'رقم', 'الملف', ':', '…'], 'pos')\n",
            "(['الحقيقة_التاسعة', 'ﺃﺗﺪﺭﻱ', 'ﻣﺎ', 'ﻣﻌﻨﻰ', 'ﻗﻮﻝ', 'ﺍﻟﻤﻈﻠﻮﻡ', ':', '\"', 'حسبي', 'الله', 'ﻭﻧﻌﻢ', 'ﺍﻟﻮﻛﻴﻞ', '\"', '؟؟', 'أﻱ', 'أﻧﻪ', 'ﻧﻘﻞ', 'ﻣﻠﻒ', 'ﺍﻟﻘﻀﻴﺔ', 'ﻣﻦ', 'ﺍﻷﺭﺽ', 'الى', 'السماء…'], 'neg')\n",
            "(['هل', 'تعلم', ':', 'واحد', 'سطايفي', 'بالضبط', 'من', '\"', 'عين', 'ولمان', '\"', 'حكمو', 'عندو', 'قناطر', 'بصلة', 'ضربوه', 'ب', 'مليون', 'بروسي', 'والشاحنة', 'سيزي', '..', 'وال…'], 'neg')\n",
            "(['لعب', 'فينا', 'البعوض', '😤'], 'neg')\n",
            "(['صباح', 'الخير', '✋'], 'neg')\n",
            "(['المسج', 'دي', 'من', 'فترة', 'و', 'كل', 'شوية', 'بدخل', 'اشوفها', 'انا', 'بحب', 'صحابي', 'جدا', 'و', 'بخاف', 'عليهم', 'و', 'بخاف', 'من', 'حبي', 'ليهم', 'دا', 'خايفة', 'ف', 'يوم', 'اتحرم', 'منهم…'], 'pos')\n",
            "(['الحياة', 'لا', 'تستحق', 'ان', 'نحزن', 'أو', 'نتألم', '..', 'فعنوان', 'الدنيا', '..(كل', 'من', 'عليها', 'فان', ')', 'وعنوان', 'الاخرة', '..(خالدين', 'فيها', 'حسنت', 'مستقرا', 'و…'], 'neg')\n",
            "(['صباح', 'الخير', 'ع', 'كل', 'من', 'ينتمى', 'للنادى', 'الاهلى', 'فقط', '❤️👏', 'إما', 'الاخرين', 'ف', 'لا', 'صباح', 'لهم', 'ولا', 'خير', '✋'], 'neg')\n",
            "(['زماان', 'ي', 'خالي', 'صدقت..', 'الله', 'يرحم', 'جدتي', 'ويسكنها', 'الجنه', 'ويطول', 'بعمرك', 'على', 'الطاعه', 'يابعد', 'راسي', '✋', '🌹'], 'neg')\n",
            "(['أنا', 'إيه', 'بالنسبه', 'لك', '؟', '🤔'], 'neg')\n",
            "(['السلام', 'عليكم', 'ورحمة', 'الله', 'وبركاته', '#مسابقه_ابوغلا', '┅━❀', '🌿🌸🌿❀━┅', 'اكتب', 'شي', 'تؤجر', 'عليه', '🥀الجوائز🥀…'], 'neg')\n",
            "(['/4/12', '-', 'الاتحاد', 'يتوج', 'بطلا', 'للدوري.', '/4/12', '-', 'الاتحاد', 'مهدد', 'بالهبوط.', 'رأيت', 'الدهر', 'مختلفا', 'يدور', '..', 'فلا', 'حزنا', 'يدو…'], 'neg')\n",
            "(['_نفسك', 'ف', 'ايه', '-مكان', 'فاضي', 'مفيهوش', 'حد', '-افضل', 'اصرخ', 'من', 'غير', 'م', 'حد', 'يكون', 'سامعني', '-اصرخ', 'بكل', 'قوتي', '💔'], 'neg')\n",
            "(['ياا', '..', 'ماا', 'وقفت', 'بوجه', 'الاياام', 'ماطحت', '✋🏽', 'ويااماا', 'وثقت', 'بنااس', 'والنااس', 'طااحوو', '😭', 'مشيت', 'بالدنياا', 'وتعلمت', '..', 'وا…'], 'neg')\n",
            "(['#مع_شركاء_القمه_الكل_رابح', 'ترقبو', 'الليلة', '#كاش💸💰', 'حفلة', 'من', 'أكبر', 'الحفلات', 'موعدنا', 'معاكم', 'اليوم', 'الساعة', ':00', '🎉', 'جائزة', '💸…'], 'neg')\n",
            "(['احنا', 'مو', 'قادرين', 'نحمي', 'الارض', 'من', 'انفسنا', 'ولا', 'نحمي', 'انفسنا', 'من', 'انفسنا،', 'الحروب', 'صارت', 'حل', 'لمشاكل', 'الدول', 'والسياسة،…'], 'pos')\n",
            "(['🔥💪⚡', '#امطار_الشرقيه', '🚫', 'لا', 'تؤجل', 'قرارك', 'بتخفيض', 'وزنك', '🔥💪⚡', 'السمنة', 'منظر', 'غير', 'جميل😔', '🔥💪⚡', 'السمنة', 'السبب', 'الاول', 'لأمراض', 'كثيرة', '🔥…'], 'neg')\n",
            "(['كلامك', 'عين', 'العقل', '👍'], 'pos')\n",
            "(['العقربه', 'من', 'اسمك', 'عوار', 'بطن', '😂'], 'pos')\n",
            "(['صدقت', 'أخي', 'الكريم..', 'إنما', 'الأمم', 'الأخلاق', 'ما', 'بقيت..', 'فإن', 'هم', 'ذهبت', 'أخلاقهم', 'ذهبوا..', 'ضاعت', 'اللغة..…'], 'pos')\n",
            "(['الحزين', 'اللي', 'ستر', 'جرحه', 'قبل', 'يشرق', 'صباحه', 'كن', 'باقي', 'له', 'من', 'الايام', '..', 'ليله', 'ما', 'رقدها', 'ليت', 'له', 'من', 'ضحكة', 'الاحباب', 'ما', 'يطلق', 'صراحه…'], 'pos')\n",
            "(['عادي', 'عندهم', 'لا', 'المدعوم', 'تكلم', 'ولا', 'الداعم', 'تاسف', '😳'], 'neg')\n",
            "(['ماعرفت', 'انام', 'زين', 'والصدارة', 'مو', 'معنا', '😏'], 'neg')\n",
            "(['❌❌تنبييه❌❌', 'الساعه', ':00', '🕛', 'تم', 'تجديد', 'الرصيد', 'للأعضاء', '🤙🏻', '#هيض_المشاعر_للدعم', 'الرجاء', 'الالتزام', 'بالشروط', '✅', 'التشييك', 'ع', 'الم…'], 'pos')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-zTWn0ZHIau"
      },
      "source": [
        "# Test data info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-KX3cU3HIau",
        "outputId": "ce5ae10e-0619-4a8c-8859-9a38ec2bab4c"
      },
      "source": [
        "print('test data info')\n",
        "test_data = pos_test_data + neg_test_data\n",
        "print('test data size', len(train_data))\n",
        "print('# of positive', len(pos_test_data))\n",
        "print('# of negative', len(neg_test_data))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test data info\n",
            "test data size 47000\n",
            "# of positive 5970\n",
            "# of negative 5781\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWcEh4hCHIav"
      },
      "source": [
        "# merging all features ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhSKI2uUHIav",
        "outputId": "883db80a-7ac8-45bb-d4b3-08792d9ec7e0"
      },
      "source": [
        "print('merging all features ... ')\n",
        "all_features = pos_train_feat + neg_train_feat + \\\n",
        "               pos_test_feat + pos_test_feat\n",
        "print('len(all_features):', len(all_features))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "merging all features ... \n",
            "len(all_features): 770508\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wok2JNd-HIaw"
      },
      "source": [
        "# Sample features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grpSOQS4HIaw",
        "outputId": "4feb9068-96e3-40a5-c308-62579e2b782d"
      },
      "source": [
        "print('{} sample features ...'.format(sample_size))\n",
        "print(random.sample(all_features, sample_size))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 sample features ...\n",
            "['أنفيلد', 'وطن', '💙🙏', '🌹', 'هه', 'النقاء', 'من', 'رتويت', 'لله💙', 'ما', 'عزله', 'حتى', 'آلهية', 'نبينا', '/', 'أقابل', 'لا…', 'آلقلب', 'ابيي', 'أصدق', 'فيارب', 'علاوي', 'القارية', 'صرف', 'صباح', 'أي', 'للناس', 'ق…', 'علينا', 'يبھجون', '،', 'الله', 'العذر', 'يطقطقون', 'دي', 'هذه', 'سنن', 'ندق', 'الساعة', 'ملگية♛', 'خفيف', 'حتى', 'وزيره', 'شاهد', 'شخصا', '😷', 'تقول', '#نبض_الامل_للدعم', 'وخير', 'من', 'صباح', 'بالاضافه', 'المفرووض', 'لم', 'حيا', '↓❁🌸', '💕', 'أحلامي', 'اللي', 'نجمتي', '😂', 'تثبت', 'هذا', 'لكل', 'في', 'بعدد', 'إن', 'مش', 'قلبك', 'حول', 'و', 'مابه', 'الجراك', 'عنها', 'يا', 'اشفي', 'يوفقك', '|', 'الأخيره', 'الوضع', 'من', 'لزقة…', 'أفضل', 'كل', 'ألف', '..', 'بعدين', 'تسمع', 'معقوله', 'نفس', 'يسالونك', 'حساب', 'جماهير', 'عين', 'لكني', '\"تلاتين', 'دهشة', 'اشلع', 'يا', 'خي']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai7ZfeM4HIay"
      },
      "source": [
        "# compute frequencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmzqS_l8HIay"
      },
      "source": [
        "all_features_count = {}\n",
        "for w in all_features:\n",
        "    all_features_count[w] = all_features_count.get(w, 0) + 1"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hdp4sszKHIaz"
      },
      "source": [
        "# Sample Frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBDRCEb-HIaz",
        "outputId": "452862b3-52be-44f0-9e64-abec9a76e969"
      },
      "source": [
        "print('sample frequencies')\n",
        "print(random.sample(list(all_features_count.items()), 30))\n",
        "word = 'في'\n",
        "print('freq of word {} is {}'.format(word, all_features_count.get(word, 0)))\n",
        "word = 'فى'\n",
        "print('freq of word {} is {}'.format(word, all_features_count.get(word, 0)))\n",
        "word = 'من'\n",
        "print('freq of word {} is {}'.format(word, all_features_count.get(word, 0)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample frequencies\n",
            "[('المماراة', 2), ('انشيلوتي', 1), ('..💙💭', 1), ('للإتي', 2), ('يداريني', 1), ('إستجابه', 5), ('ده؟😁💔🚶', 1), ('حرووف', 2), ('لوتسوي', 1), ('والنسيم', 1), ('ورديتها', 1), ('اليوسى', 1), ('وعندما', 30), ('نجاة', 1), ('القصة؟', 1), ('k+', 1), ('يغلبني', 4), ('للمبتسمين', 7), ('ديال', 2), ('اللامبالاه', 2), ('سعه،', 1), ('ابيج', 2), ('متانة..', 1), ('DADDY', 1), ('حسافة', 7), ('ياضحكھ', 1), ('ييما', 1), ('ورضا..', 1), ('عمكم', 2), ('(النسخة', 1)]\n",
            "freq of word في is 9550\n",
            "freq of word فى is 220\n",
            "freq of word من is 12655\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGmGhS9PHIa0"
      },
      "source": [
        "# Compute Threshold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZvZ-8I-HIa1",
        "outputId": "6b59dc43-6230-474a-8857-3f199dc4d3c7"
      },
      "source": [
        "print('size of training data:',  len(train_data))\n",
        "min_df = int(0.001 * len(train_data))\n",
        "max_df = int(0.98 * len(train_data))\n",
        "print('min document frequency:', min_df)\n",
        "print('max document frequency:', max_df)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "size of training data: 47000\n",
            "min document frequency: 47\n",
            "max document frequency: 46060\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlxK2G8EHIa1"
      },
      "source": [
        "# Selecting Features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLEKKqnRHIa2",
        "outputId": "fe796e6b-59c9-426c-c75b-8dac5b969439"
      },
      "source": [
        "# remove features that have frequency below/above the threshold\n",
        "my_features = set([word for word, freq in all_features_count.items() if  max_df > freq > min_df ])\n",
        "print(len(my_features), 'are kept out of', len(all_features))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1961 are kept out of 770508\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxlysuoWHIa2"
      },
      "source": [
        "# Sample of selected features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdIChUd7HIa3",
        "outputId": "f6434f2b-e5e8-47f7-e344-d44692501867"
      },
      "source": [
        "print('{} sample of selected features:'.format(sample_size))\n",
        "print(random.sample(list(my_features), sample_size))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 sample of selected features:\n",
            "['نبينا', '×', 'فضلت', '😳', 'تبارك', 'تعال', 'ﻣﻦ', 'والسرور', 'من', 'والشوق', 'احب', 'عمرنا', 'الذنوب', 'عندنا', 'أسعد', 'الحلوه', 'يد', 'المهم', 'الفجر', 'نفرح', 'اكتب', 'امطار', 'اليوم', 'كانوا', 'وكيف', 'يرد', 'خبر', 'القصبي', 'ماوحشتك', 'القدر', 'وفوق', '؟!', 'فيك', 'إشراقة', 'والي', 'رحمتك', 'الجمعه', 'رجال', '😕', 'اجازه', 'تركت', '🎀', 'السحب', 'لين', 'معاه', 'يقولون', 'تصير', 'التغريده', '#الفيفا_يرد_احتجاج_الهلال_صحيح', 'يارب', 'برحيل', '#زلزل_الملعب_نصرنا_بيلعب', 'به', 'يتحدثون', 'وعن', 'الهم', \"'\", 'صارت', 'قالوا', 'العالم', 'بسبب', 'المعروف', 'الطيب', 'بيني', 'كم', 'القيم', 'لك؟', 'فلا', 'فيه', 'موسم', '#الأهلي_الهلال', 'يابو', 'بلغنا', 'داود', 'التي', 'ترد', 'ال…', 'قدر', 'توقفون', 'مدريد', 'وليس', '☆', 'الحي', 'صبح', 'كثير', 'حسين', 'رجعنا', 'ﻭﻓﻲ', 'كان', 'قبل', 'ღ♬', 'عشي', 'وطبق', 'العسل', 'شوي', 'او', '👌', 'العافيه', 'منه', 'المتاريس']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmrnH9THHIa3"
      },
      "source": [
        "# generating features for training documents ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydrCt9YuHIa3"
      },
      "source": [
        "feature_sets = [(document_features(d, my_features), c) for (d, c) in train_data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfpSybSVHIa4"
      },
      "source": [
        "# training ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSEJ1GZ2HIa4"
      },
      "source": [
        "classifier = nltk.NaiveBayesClassifier.train(feature_sets)\n",
        "print('training is done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b-HJZwzHIa5"
      },
      "source": [
        "# Most informative features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7OQaepCHIa5"
      },
      "source": [
        "classifier.show_most_informative_features(40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8cg2toRHIa6"
      },
      "source": [
        "# generating features for test documents ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QsrbtMxHIa6"
      },
      "source": [
        "test_features = [(document_features(d, my_features), c) for (d, c) in test_data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qYi7NIUHIa6"
      },
      "source": [
        "# classify test instances "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFM5M48gHIa7"
      },
      "source": [
        "ref_sets = collections.defaultdict(set)\n",
        "test_sets = collections.defaultdict(set)\n",
        "\n",
        "for i, (feats, label) in enumerate(test_features):\n",
        "    ref_sets[label].add(i)\n",
        "    observed = classifier.classify(feats)\n",
        "    test_sets[observed].add(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h099w5PpHIa7"
      },
      "source": [
        "# Results "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNU9Uu55HIa8"
      },
      "source": [
        "print('accuracy: ', nltk.classify.accuracy(classifier, test_features))\n",
        "print('pos precision: ', precision(ref_sets['pos'], test_sets['pos']))\n",
        "print('pos recall:', recall(ref_sets['pos'], test_sets['pos']))\n",
        "print('neg precision: ', precision(ref_sets['neg'], test_sets['neg']))\n",
        "print('neg recall:', recall(ref_sets['neg'], test_sets['neg']))\n",
        "print('positive f-score:', f_measure(ref_sets['pos'], test_sets['pos']))\n",
        "print('negative f-score:', f_measure(ref_sets['neg'], test_sets['neg']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjhRYa49HIa8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}